{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3a7b75c8-77e6-4f8a-b6fa-43e26160a076",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import libraries\n",
    "import requests\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import os\n",
    "import html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9dfacd7b-e322-4702-a42d-fc89b2d1e45f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check what to do with validations-reseau.csv?\n",
    "#Need PCA bc units are different for variabless"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f252aae8-65d5-4975-84a6-820929be139b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Download dataset\n",
    "df_lines = pd.read_csv(\"./Online_Data/referentiel-des-lignes.csv\", sep=';')\n",
    "stops_data = pd.read_csv(\"./Online_Data/arrets.csv\", sep=';')\n",
    "\n",
    "trafic2023_ratio = pd.read_csv(\"./Online_Data/validations-1er-semestre.csv\", sep=';')\n",
    "trafic2023_raw = pd.read_csv(\"./Online_Data/validations-reseau.csv\", sep=\";\")\n",
    "\n",
    "hourly_weather = pd.read_csv(\"./Online_Data/Weather/hourly_weather.csv\")\n",
    "minutely_15_weather = pd.read_csv(\"./Online_Data/Weather/minutely_15_weather.csv\")\n",
    "\n",
    "delays_metro = pd.read_csv(\"./Collected_Data/metro_delays.csv\")\n",
    "delays_rer = pd.read_csv(\"./Collected_Data/rail_delays.csv\")\n",
    "onTime_metro = pd.read_csv(\"./Collected_Data/metro_onTime.csv\")\n",
    "onTime_rer = pd.read_csv(\"./Collected_Data/rail_onTime.csv\")\n",
    "\n",
    "metro_incident = pd.read_csv(\"./Collected_Data/metro_line_reports.csv\")\n",
    "rer_incident = pd.read_csv(\"./Collected_Data/rer_line_reports.csv\")\n",
    "\n",
    "df_holidays = pd.read_csv(\"./Online_Data/vacances-scolaires-par-zone.csv\", sep = \";\")\n",
    "df_bank_holidays = pd.read_csv(\"./Online_Data/jours_feries_metropole.csv\", sep=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e1d30d5c-4b20-4ae6-b52b-4ed202aa4cf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create filter\n",
    "stops_filter = [22086, 463013, 22136, 462993, 21964, 462969, 22125, 463113, 41295, 473921, 473993, 41354, 474060, 474061]\n",
    "name_filter = [\"CH.D.G.ETOILE\", \"CHATELET\", \"SAINT-LAZARE\",\"ST-GERM.D.PRES\", \"BLANCHE\", \"AVENUE DU PRESIDENT KENNEDY\", \"BUNO GIRONVILLE\", \"MASSY PALAISEAU\"]\n",
    "lines_filter = [\"C01371\", \"C01372\", \"C01374\" ,\" C01382\", \"C01742\", \"C01743\", \"C01727\", \"C0172\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b5fc7e94",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Clean datasets\n",
    "line_refs = df_lines[(~df_lines['TransportSubmode'].isin(['suburbanRailway', 'regionalRail', 'railShuttle']))]\n",
    "line_refs = line_refs[['ID_Line', 'TransportMode', 'Name_Line']]\n",
    "line_refs = line_refs.sort_values(by='ID_Line')\n",
    "\n",
    "stops_data = stops_data[stops_data['ArRType'].isin(['metro', 'rail'])]\n",
    "stops_data = stops_data.sort_values(by=['ArRType', 'ArRId'])\n",
    "stops_data = stops_data[['ArRId', 'ArRName', 'ArRType', 'ArRTown']]\n",
    "\n",
    "def cleaning_message(df):\n",
    "    df['ref'] = df['ref'].str.replace('stop_point:IDFM:', '', regex=False)\n",
    "    df['ref'] = df['ref'].str.replace('line:IDFM:', '', regex=False)\n",
    "    df['ref'] = df['ref'].str.replace('stop_area:IDFM:', '', regex=False)\n",
    "    df['message_text'] = df['message_text'].str.replace('<p>', '', regex=False)\n",
    "    df['message_text'] = df['message_text'].str.replace('<br>', '', regex=False) \n",
    "    df = df[df[\"channel_name\"].isin([\"moteur\"])]\n",
    "\n",
    "cleaning_message(metro_incident)\n",
    "cleaning_message(rer_incident)\n",
    "\n",
    "metro_incident['message_text'] = metro_incident['message_text'].str.split('</p>').str[0]\n",
    "metro_incident.drop_duplicates(subset=[\"disruption_id\"], inplace=True)\n",
    "metro_incident.drop(['tags', 'category', 'updated_at', 'channel_name'], axis=1, inplace=True) \n",
    "\n",
    "rer_incident['message_text'] = rer_incident['message_text'].str.replace('</p>', '', regex=False)\n",
    "rer_incident['message_text'] = rer_incident['message_text'].apply(html.unescape)\n",
    "rer_incident.drop_duplicates(subset=[\"disruption_id\"], inplace=True)\n",
    "rer_incident.drop(['tags', 'category', 'updated_at', 'channel_name'], axis=1, inplace=True) \n",
    "\n",
    "def refs(df):\n",
    "    df['line_ref'] = df['line_ref'].str.replace('STIF:Line::', '', regex=False).str.rstrip(':')\n",
    "    df['stop_reference'] = pd.to_numeric(df['stop_reference'], errors='coerce')\n",
    "    df['stop_reference'] = df['stop_reference'].fillna(0).astype('int64')\n",
    "\n",
    "refs(delays_metro)\n",
    "refs(delays_rer)\n",
    "refs(onTime_metro)\n",
    "refs(onTime_rer)\n",
    "\n",
    "delays_metro.drop(['scheduled_arrival','scheduled_departure','arrival_difference','departure_difference'], axis=1, inplace=True)\n",
    "onTime_metro.drop(['scheduled_arrival','scheduled_departure','arrival_difference','departure_difference'], axis=1, inplace=True)\n",
    "\n",
    "mapping = dict(zip(name_filter, stops_filter))\n",
    "trafic2023_ratio['LIBELLE_ARRET_REA'] = trafic2023_ratio['LIBELLE_ARRET'].replace(mapping) #issue -  incorrect mapping? stop number refers to wrong stop\n",
    "trafic2023_ratio = trafic2023_ratio[trafic2023_ratio['LIBELLE_ARRET_REA'].isin(stops_filter)]\n",
    "\n",
    "#trafic2023_raw['LIBELLE_ARRET_REA'] = trafic2023_raw['LIBELLE_ARRET'].replace(mapping) #don't need anymore?\n",
    "#trafic2023_raw.drop([\"lda\"],axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ba355b65",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Holidays and day types\n",
    "df_bank_holidays[\"Date\"] = pd.to_datetime(df_bank_holidays[\"date\"])\n",
    "df_holidays[\"Date\"] = pd.to_datetime(df_holidays[\"Date\"])\n",
    "\n",
    "df_with_holidays = df_holidays.merge(df_bank_holidays, on = \"Date\", how = \"left\")\n",
    "df_with_holidays = df_with_holidays.sort_values(by='Date')\n",
    "\n",
    "df_with_holidays['holiday_type'] = df_with_holidays['nom_jour_ferie'].combine_first(df_with_holidays['Nom de la période'])\n",
    "df_with_holidays['day_of_week'] = df_with_holidays['Date'].dt.weekday\n",
    "\n",
    "def classify_day(row):\n",
    "    if row['day_of_week'] < 5:  # Weekdays (Monday to Friday)\n",
    "        if pd.notna(row['holiday_type']):\n",
    "            return 'JOVS'  # Weekday with a holiday\n",
    "        else:\n",
    "            return 'JOHV'  # Weekday without a holiday\n",
    "    elif row['day_of_week'] == 5:  # Saturday\n",
    "        if pd.notna(row['holiday_type']):\n",
    "            return 'SAVS'  # Saturday with a holiday\n",
    "        else:\n",
    "            return 'SAHV'  # Saturday without a holiday\n",
    "    elif row['day_of_week'] == 6:  # Sunday\n",
    "        return 'DIJFP'  # Sunday (always labeled DIJFP)\n",
    "    return None\n",
    "\n",
    "df_with_holidays['day_type'] = df_with_holidays.apply(classify_day, axis=1)\n",
    "df_with_holidays[\"is_bank_holiday\"] = (df_with_holidays[\"nom_jour_ferie\"]).notna().astype(int)\n",
    "df_with_holidays[\"is_holiday\"] = (df_with_holidays[\"Nom de la période\"]).notna().astype(int)\n",
    "df_with_holidays[\"saturday\"]= (df_with_holidays[\"Date\"].dt.weekday == 5).astype(int)\n",
    "df_with_holidays[\"sunday\"]=  (df_with_holidays[\"Date\"].dt.weekday == 6).astype(int)\n",
    "df_with_holidays[\"is_weekend\"]= df_with_holidays[\"Date\"].dt.weekday.isin([5,6]).astype(int)\n",
    "df_with_holidays[\"is_weekend_or_bank_holiday\"] = df_with_holidays[[\"is_weekend\", \"is_bank_holiday\"]].max(axis=1)\n",
    "\n",
    "df_with_holidays.drop(['timestamp_unix', 'date', 'annee', 'zone', 'Calendrier Zone A', 'Calendrier Zone B', 'Calendrier Zone C'], axis=1, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "69302686",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Filter dataset\n",
    "stops_data = stops_data[stops_data['ArRId'].isin(stops_filter)]\n",
    "\n",
    "metro_incident = metro_incident[\n",
    "    metro_incident['ref'].isin(map(str, stops_filter + lines_filter))\n",
    "]\n",
    "\n",
    "rer_incident = rer_incident[\n",
    "    rer_incident['ref'].isin(map(str, stops_filter + lines_filter))\n",
    "]\n",
    "\n",
    "trafic2023_ratio[['start_hour', 'end_hour']] = trafic2023_ratio['TRNC_HORR_60'].str.extract(r'(\\d+)H-(\\d+)H').dropna().astype(int)\n",
    "trafic2023_ratio.drop(['CODE_STIF_RES', 'CODE_STIF_ARRET', 'LIBELLE_ARRET_REA', 'lda'], axis=1, inplace=True)\n",
    "\n",
    "trafic2023_ratio_rer = trafic2023_ratio[trafic2023_ratio['CODE_STIF_TRNS'].isin([810, 800])]\n",
    "trafic2023_ratio_metro = trafic2023_ratio[trafic2023_ratio['CODE_STIF_TRNS'].isin([100])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "aeb54104",
   "metadata": {},
   "outputs": [],
   "source": [
    "#don't need?\n",
    "#trafic2023_raw = trafic2023_raw[trafic2023_raw['LIBELLE_ARRET_REA'].isin(stops_filter)] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "afadb29f-88b3-4e5e-a454-3d0d6cc7fd77",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Merge datasets\n",
    "merged_metro = pd.concat([delays_metro, onTime_metro], ignore_index=True)\n",
    "merged_metro = pd.merge(merged_metro, stops_data, left_on='stop_reference', right_on='ArRId')\n",
    "merged_metro.drop(['ArRId', 'ArRName', 'ArRType', 'transport_mode', 'recorded_at_time'], axis=1, inplace=True) \n",
    "\n",
    "merged_rer = pd.concat([delays_rer, onTime_rer], ignore_index=True)\n",
    "merged_rer = pd.merge(merged_rer, stops_data, left_on='stop_reference', right_on='ArRId')\n",
    "merged_rer.drop(['ArRId', 'ArRName', 'ArRType', 'transport_mode', 'recorded_at_time'],  axis=1, inplace=True)\n",
    "\n",
    "def date_format(df):\n",
    "    df['timestamp'] = pd.to_datetime(df['timestamp'].str.replace('Z', ''), errors='coerce')\n",
    "    df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
    "    df['real_arrival'] = pd.to_datetime(df['real_arrival']).dt.strftime('%Y-%m-%d %H:%M:%S')\n",
    "    df['real_arrival'] = pd.to_datetime(df['real_arrival'])\n",
    "    df['real_departure'] = pd.to_datetime(df['real_departure']).dt.strftime('%Y-%m-%d %H:%M:%S')\n",
    "    df['real_departure'] = pd.to_datetime(df['real_departure'])\n",
    "\n",
    "\n",
    "def format_rer(df):\n",
    "    df['scheduled_arrival'] = pd.to_datetime(df['scheduled_arrival']).dt.strftime('%Y-%m-%d %H:%M:%S')\n",
    "    df['scheduled_arrival'] = pd.to_datetime(df['scheduled_arrival'])\n",
    "    df['scheduled_departure'] = pd.to_datetime(df['scheduled_departure']).dt.strftime('%Y-%m-%d %H:%M:%S')\n",
    "    df['scheduled_departure'] = pd.to_datetime(df['scheduled_departure'])\n",
    "    df.loc[(df['arrival_difference'] >= 5) | (df['departure_difference'] >= 5), 'departure_status'] = 'delayed'\n",
    "    df['nearest_datetime'] = df['scheduled_arrival'].combine_first(df['scheduled_departure'])\n",
    "\n",
    "date_format(merged_metro)\n",
    "date_format(merged_rer)\n",
    "format_rer(merged_rer)\n",
    "\n",
    "merged_metro = merged_metro.sort_values(by='real_arrival')\n",
    "merged_rer = merged_rer.sort_values(by='nearest_datetime')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2cd4d3e7-3e72-4853-9080-8995cf45bea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Clean Gas Price data\n",
    "def clean_gas_data(file_path, fuel_type):\n",
    "    df = pd.read_csv(file_path, skiprows=3, sep=';')\n",
    "    df.rename(columns={df.columns[1]: f'{fuel_type}'}, inplace=True)\n",
    "    df = df.iloc[:, :-1]\n",
    "    return df\n",
    "    \n",
    "gas_95_df = clean_gas_data(\"./Online_Data/Gas/octane_95.csv\", \"95\")\n",
    "gas_98_df = clean_gas_data(\"./Online_Data/Gas/octane_98.csv\", \"98\")\n",
    "gas_e10_df = clean_gas_data(\"./Online_Data/Gas/95-E10.csv\", \"E10\")\n",
    "gazole_df = clean_gas_data(\"./Online_Data/Gas/gazole.csv\", \"gazole\")\n",
    "\n",
    "merged_rer['Période'] = pd.to_datetime(merged_rer['nearest_datetime']).dt.strftime('%Y-%m')\n",
    "merged_rer = merged_rer.merge(gas_95_df, on='Période', how='left')\n",
    "merged_rer = merged_rer.merge(gas_98_df, on='Période', how='left')\n",
    "merged_rer = merged_rer.merge(gas_e10_df, on='Période', how='left')\n",
    "merged_rer = merged_rer.merge(gazole_df, on='Période', how='left')\n",
    "\n",
    "merged_metro.drop(columns=['timestamp'], inplace=True)\n",
    "merged_rer.drop(columns=['Période', 'timestamp'], inplace=True)\n",
    "merged_rer = merged_rer[~merged_rer.duplicated(subset=merged_rer.columns.difference(['delayed_status']).tolist(), keep='last')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "577774b0-7de7-4753-a1c0-4c3868fad8dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Weather Variable\n",
    "\n",
    "#hourly_weather['date'] = pd.to_datetime(hourly_weather['date']) - don't need hourly?\n",
    "minutely_15_weather['date'] = pd.to_datetime(minutely_15_weather['date'])\n",
    "\n",
    "merged_metro = pd.merge_asof(merged_metro, \n",
    "                   minutely_15_weather, \n",
    "                   left_on='real_arrival', \n",
    "                   right_on='date', \n",
    "                   direction='nearest')\n",
    "\n",
    "merged_rer = pd.merge_asof(merged_rer, \n",
    "                   minutely_15_weather, \n",
    "                   left_on='nearest_datetime', \n",
    "                   right_on='date', \n",
    "                   direction='nearest')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6b510369",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Add holidays\n",
    "merged_rer['day'] = pd.to_datetime(merged_rer['nearest_datetime']).dt.normalize()\n",
    "merged_rer = merged_rer.merge(df_with_holidays, left_on='day', right_on='Date', how='left')\n",
    "merged_rer['hour'] = merged_rer['nearest_datetime'].dt.hour.astype('int64')\n",
    "\n",
    "merged_metro['day'] = pd.to_datetime(merged_metro['real_arrival']).dt.normalize()\n",
    "merged_metro = merged_metro.merge(df_with_holidays, left_on='day', right_on='Date', how='left')\n",
    "merged_metro['hour'] = merged_metro['real_arrival'].dt.hour.astype('int64')\n",
    "\n",
    "merged_metro = merged_metro.sort_values(by='real_arrival')\n",
    "merged_rer = merged_rer.sort_values(by='nearest_datetime')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "effacf6a-b7b8-44b7-9c61-45b2121dc758",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split by stop\n",
    "CDG = merged_metro[merged_metro['stop_name'].isin(['Charles de Gaulle-Etoile'])]\n",
    "SGP = merged_metro[merged_metro['stop_name'].isin(['Saint-Germain des Prés'])]\n",
    "BL = merged_metro[merged_metro['stop_name'].isin(['Blanche'])]\n",
    "SL = merged_metro[merged_metro['stop_name'].isin(['Saint-Lazare'])]\n",
    "APK = merged_rer[merged_rer['stop_name'].isin(['Avenue du Président Kennedy'])]\n",
    "CLH = merged_rer[merged_rer['stop_name'].isin(['Châtelet - Les Halles'])]\n",
    "GBG = merged_rer[merged_rer['stop_name'].isin(['Gare de Buno Gironville'])]\n",
    "MP = merged_rer[merged_rer['stop_name'].isin(['Massy - Palaiseau'])]\n",
    "\n",
    "CDG = CDG.drop_duplicates(subset=['destination_name', 'real_arrival'], keep='first')\n",
    "SGP = SGP.drop_duplicates(subset=['destination_name', 'real_arrival'], keep='first')\n",
    "BL = BL.drop_duplicates(subset=['destination_name', 'real_arrival'], keep='first')\n",
    "SL = SL.drop_duplicates(subset=['destination_name', 'real_arrival'], keep='first')\n",
    "\n",
    "APK = APK.drop_duplicates(subset=['destination_name', 'scheduled_arrival'], keep='first')\n",
    "CLH = CLH.drop_duplicates(subset=['destination_name', 'scheduled_arrival'], keep='first')\n",
    "GBG = GBG.drop_duplicates(subset=['destination_name', 'scheduled_arrival'], keep='first')\n",
    "MP = MP.drop_duplicates(subset=['destination_name', 'scheduled_arrival'], keep='first')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed325487",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Add traffic data\n",
    "CDG_traffic = trafic2023_ratio_metro[trafic2023_ratio_metro['LIBELLE_ARRET'] == 'CH.D.G.ETOILE']\n",
    "SGP_traffic = trafic2023_ratio_metro[trafic2023_ratio_metro['LIBELLE_ARRET'] == 'ST-GERM.D.PRES']\n",
    "BL_traffic = trafic2023_ratio_metro[trafic2023_ratio_metro['LIBELLE_ARRET'] == 'BLANCHE']\n",
    "SL_traffic = trafic2023_ratio_metro[trafic2023_ratio_metro['LIBELLE_ARRET'] == 'SAINT-LAZARE']\n",
    "APK_traffic = trafic2023_ratio_rer[trafic2023_ratio_rer['LIBELLE_ARRET'] == 'AVENUE DU PRESIDENT KENNEDY']\n",
    "CLH_traffic = trafic2023_ratio_rer[trafic2023_ratio_rer['LIBELLE_ARRET'] == 'CHATELET']\n",
    "GBG_traffic = trafic2023_ratio_rer[trafic2023_ratio_rer['LIBELLE_ARRET'] == 'BUNO GIRONVILLE']\n",
    "MP_traffic = trafic2023_ratio_rer[trafic2023_ratio_rer['LIBELLE_ARRET'] == 'MASSY PALAISEAU']\n",
    "\n",
    "CDG = CDG.merge(CDG_traffic, left_on=['day_type', 'hour'], right_on=['CAT_JOUR', 'start_hour'], how='left')\n",
    "SGP = SGP.merge(SGP_traffic, left_on=['day_type', 'hour'], right_on=['CAT_JOUR', 'start_hour'], how='left')\n",
    "BL = BL.merge(BL_traffic, left_on=['day_type', 'hour'], right_on=['CAT_JOUR', 'start_hour'], how='left')\n",
    "SL = SL.merge(SL_traffic, left_on=['day_type', 'hour'], right_on=['CAT_JOUR', 'start_hour'], how='left')\n",
    "APK = APK.merge(APK_traffic, left_on=['day_type', 'hour'], right_on=['CAT_JOUR', 'start_hour'], how='left')\n",
    "CLH = CLH.merge(CLH_traffic, left_on=['day_type', 'hour'], right_on=['CAT_JOUR', 'start_hour'], how='left')\n",
    "GBG = GBG.merge(GBG_traffic, left_on=['day_type', 'hour'], right_on=['CAT_JOUR', 'start_hour'], how='left')\n",
    "MP = MP.merge(MP_traffic, left_on=['day_type', 'hour'], right_on=['CAT_JOUR', 'start_hour'], how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0ad358c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Drop uneccesary columns - can be edited\n",
    "dfs = [CDG, SGP, BL, SL, APK, CLH, GBG, MP]\n",
    "\n",
    "for df in dfs:\n",
    "    df.drop(columns=['stop_reference', 'stop_name', 'line_ref', 'day', 'date', 'Nom de la période', 'nom_jour_ferie', 'holiday_type', 'hour', 'CODE_STIF_TRNS', 'LIBELLE_ARRET', 'CAT_JOUR', 'TRNC_HORR_60', 'start_hour', 'end_hour'], inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9dc91dfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#To csv\n",
    "for df, name in zip(dfs, ['CDG', 'SGP', 'BL', 'SL', 'APK', 'CLH', 'GBG', 'MP']):\n",
    "    df.to_csv(f'{name}.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1fa3797",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
