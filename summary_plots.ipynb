{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fe66437f-e39f-4617-bc3d-f91d49b204e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add number of delays per day over full dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "389988fd-7d4d-46f6-a4bd-5cfe39210089",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mca\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import requests\n",
    "from datetime import datetime, timedelta\n",
    "import os\n",
    "import html\n",
    "import re\n",
    "from dateutil.parser import parse\n",
    "import warnings\n",
    "#warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split, ShuffleSplit, KFold, cross_val_score, GridSearchCV, StratifiedKFold"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e4a97fb",
   "metadata": {},
   "source": [
    "## Data loading "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b160969b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_lines = pd.read_csv(\"./Online_Data/referentiel-des-lignes.csv\", sep=';')\n",
    "stops_data = pd.read_csv(\"./Online_Data/arrets.csv\", sep=';')\n",
    "\n",
    "trafic2023_ratio = pd.read_csv(\"./Online_Data/validations-1er-semestre.csv\", sep=';')\n",
    "trafic2023_raw = pd.read_csv(\"./Online_Data/validations-reseau.csv\", sep=\";\")\n",
    "\n",
    "hourly_weather = pd.read_csv(\"./Online_Data/Weather/hourly_weather.csv\")\n",
    "minutely_15_weather = pd.read_csv(\"./Online_Data/Weather/minutely_15_weather.csv\")\n",
    "\n",
    "metro_incident = pd.read_csv(\"./Collected_Data/metro_line_reports.csv\")\n",
    "rer_incident = pd.read_csv(\"./Collected_Data/rer_line_reports.csv\")\n",
    "\n",
    "df_holidays = pd.read_csv(\"./Online_Data/vacances-scolaires-par-zone.csv\", sep = \";\")\n",
    "df_bank_holidays = pd.read_csv(\"./Online_Data/jours_feries_metropole.csv\", sep=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "75b918a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "delays_metro = pd.read_csv(\"./Collected_Data/metro_delays.csv\")\n",
    "delays_rer = pd.read_csv(\"./Collected_Data/rail_delays.csv\")\n",
    "onTime_metro = pd.read_csv(\"./Collected_Data/metro_onTime.csv\", low_memory=False)\n",
    "onTime_rer = pd.read_csv(\"./Collected_Data/rail_onTime.csv\", low_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d9b64785",
   "metadata": {},
   "outputs": [],
   "source": [
    "delays_metro2 = pd.read_csv(\"./Collected_Data/metro_delays2.csv\")\n",
    "delays_rer2 = pd.read_csv(\"./Collected_Data/rail_delays2.csv\")\n",
    "onTime_metro2 = pd.read_csv(\"./Collected_Data/metro_onTime2.csv\", low_memory=False)\n",
    "onTime_rer2 = pd.read_csv(\"./Collected_Data/rail_onTime2.csv\", low_memory=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f66d4ea1",
   "metadata": {},
   "source": [
    "## Data cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5e1a96d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "delays_metro = pd.concat([delays_metro, delays_metro2], ignore_index=True)\n",
    "delays_rer = pd.concat([delays_rer, delays_rer2], ignore_index=True)\n",
    "onTime_metro = pd.concat([onTime_metro, onTime_metro2], ignore_index=True)\n",
    "onTime_rer = pd.concat([onTime_rer, onTime_rer2], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4f1705ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create filter\n",
    "stops_filter = [22086, 463013, 22136, 462993, 21964, 462969, 22125, 463113, 41295, 473921, 473993, 41354, 474060, 474061]\n",
    "name_filter = [\"CH.D.G.ETOILE\", \"CHATELET\", \"SAINT-LAZARE\",\"ST-GERM.D.PRES\", \"BLANCHE\", \"AVENUE DU PRESIDENT KENNEDY\", \"BUNO GIRONVILLE\", \"MASSY PALAISEAU\"]\n",
    "lines_filter = [\"C01371\", \"C01372\", \"C01374\" ,\" C01382\", \"C01742\", \"C01743\", \"C01727\", \"C0172\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3d319d44",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Clean datasets\n",
    "line_refs = df_lines[(~df_lines['TransportSubmode'].isin(['suburbanRailway', 'regionalRail', 'railShuttle']))]\n",
    "line_refs = line_refs[['ID_Line', 'TransportMode', 'Name_Line']]\n",
    "line_refs = line_refs.sort_values(by='ID_Line')\n",
    "\n",
    "stops_data = stops_data[stops_data['ArRType'].isin(['metro', 'rail'])]\n",
    "stops_data = stops_data.sort_values(by=['ArRType', 'ArRId'])\n",
    "stops_data = stops_data[['ArRId', 'ArRName', 'ArRType', 'ArRTown']]\n",
    "\n",
    "def cleaning_message(df):\n",
    "    df['ref'] = df['ref'].str.replace('stop_point:IDFM:', '', regex=False)\n",
    "    df['ref'] = df['ref'].str.replace('line:IDFM:', '', regex=False)\n",
    "    df['ref'] = df['ref'].str.replace('stop_area:IDFM:', '', regex=False)\n",
    "    df['message_text'] = df['message_text'].str.replace('<p>', '', regex=False)\n",
    "    df['message_text'] = df['message_text'].str.replace('<br>', '', regex=False) \n",
    "    df = df[df[\"channel_name\"].isin([\"moteur\"])]\n",
    "\n",
    "cleaning_message(metro_incident)\n",
    "cleaning_message(rer_incident)\n",
    "\n",
    "metro_incident['message_text'] = metro_incident['message_text'].str.split('</p>').str[0]\n",
    "metro_incident.drop_duplicates(subset=[\"disruption_id\"], inplace=True)\n",
    "metro_incident.drop(['tags', 'category', 'updated_at', 'channel_name'], axis=1, inplace=True) \n",
    "\n",
    "rer_incident['message_text'] = rer_incident['message_text'].str.replace('</p>', '', regex=False)\n",
    "rer_incident['message_text'] = rer_incident['message_text'].apply(html.unescape)\n",
    "rer_incident.drop_duplicates(subset=[\"disruption_id\"], inplace=True)\n",
    "rer_incident.drop(['tags', 'category', 'updated_at', 'channel_name'], axis=1, inplace=True) \n",
    "\n",
    "def refs(df):\n",
    "    df['line_ref'] = df['line_ref'].str.replace('STIF:Line::', '', regex=False).str.rstrip(':')\n",
    "    df['stop_reference'] = pd.to_numeric(df['stop_reference'], errors='coerce')\n",
    "    df['stop_reference'] = df['stop_reference'].fillna(0).astype('int64')\n",
    "\n",
    "refs(delays_metro)\n",
    "refs(delays_rer)\n",
    "refs(onTime_metro)\n",
    "refs(onTime_rer)\n",
    "\n",
    "delays_metro.drop(['scheduled_arrival','scheduled_departure','arrival_difference','departure_difference'], axis=1, inplace=True)\n",
    "onTime_metro.drop(['scheduled_arrival','scheduled_departure','arrival_difference','departure_difference'], axis=1, inplace=True)\n",
    "\n",
    "#mapping = dict(zip(name_filter, stops_filter))\n",
    "#trafic2023_ratio['LIBELLE_ARRET_REA'] = trafic2023_ratio['LIBELLE_ARRET'].replace(mapping) #issue -  incorrect mapping? stop number refers to wrong stop\n",
    "#trafic2023_ratio = trafic2023_ratio[trafic2023_ratio['LIBELLE_ARRET_REA'].isin(stops_filter)]\n",
    "\n",
    "#trafic2023_raw['LIBELLE_ARRET_REA'] = trafic2023_raw['LIBELLE_ARRET'].replace(mapping) #don't need anymore?\n",
    "#trafic2023_raw.drop([\"lda\"],axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f25fe7e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "trafic2023_ratio_rer = trafic2023_ratio[trafic2023_ratio['CODE_STIF_TRNS'].isin([810, 800])]\n",
    "trafic2023_ratio_metro = trafic2023_ratio[trafic2023_ratio['CODE_STIF_TRNS'].isin([100])]\n",
    "trafic2023_raw_rer = trafic2023_raw[trafic2023_raw['CODE_STIF_TRNS'].isin([810, 800])]\n",
    "trafic2023_raw_metro = trafic2023_raw[trafic2023_raw['CODE_STIF_TRNS'].isin([100])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3d92f3c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Holidays and day types\n",
    "df_bank_holidays[\"Date\"] = pd.to_datetime(df_bank_holidays[\"date\"])\n",
    "df_holidays[\"Date\"] = pd.to_datetime(df_holidays[\"Date\"])\n",
    "\n",
    "df_with_holidays = df_holidays.merge(df_bank_holidays, on = \"Date\", how = \"left\")\n",
    "df_with_holidays = df_with_holidays.sort_values(by='Date')\n",
    "\n",
    "df_with_holidays['holiday_type'] = df_with_holidays['nom_jour_ferie'].combine_first(df_with_holidays['Nom de la période'])\n",
    "df_with_holidays['day_of_week'] = df_with_holidays['Date'].dt.weekday\n",
    "\n",
    "def classify_day(row):\n",
    "    if row['day_of_week'] < 5:  # Weekdays (Monday to Friday)\n",
    "        if pd.notna(row['holiday_type']):\n",
    "            return 'JOVS'  # Weekday with a holiday\n",
    "        else:\n",
    "            return 'JOHV'  # Weekday without a holiday\n",
    "    elif row['day_of_week'] == 5:  # Saturday\n",
    "        if pd.notna(row['holiday_type']):\n",
    "            return 'SAVS'  # Saturday with a holiday\n",
    "        else:\n",
    "            return 'SAHV'  # Saturday without a holiday\n",
    "    elif row['day_of_week'] == 6:  # Sunday\n",
    "        return 'DIJFP'  # Sunday (always labeled DIJFP)\n",
    "    return None\n",
    "\n",
    "df_with_holidays['day_type'] = df_with_holidays.apply(classify_day, axis=1)\n",
    "df_with_holidays[\"is_bank_holiday\"] = (df_with_holidays[\"nom_jour_ferie\"]).notna().astype(int)\n",
    "df_with_holidays[\"is_holiday\"] = (df_with_holidays[\"Nom de la période\"]).notna().astype(int)\n",
    "df_with_holidays[\"saturday\"]= (df_with_holidays[\"Date\"].dt.weekday == 5).astype(int)\n",
    "df_with_holidays[\"sunday\"]=  (df_with_holidays[\"Date\"].dt.weekday == 6).astype(int)\n",
    "df_with_holidays[\"is_weekend\"]= df_with_holidays[\"Date\"].dt.weekday.isin([5,6]).astype(int)\n",
    "df_with_holidays[\"is_weekend_or_bank_holiday\"] = df_with_holidays[[\"is_weekend\", \"is_bank_holiday\"]].max(axis=1)\n",
    "\n",
    "df_with_holidays.drop(['timestamp_unix', 'date', 'annee', 'zone', 'Calendrier Zone A', 'Calendrier Zone B', 'Calendrier Zone C'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7ded7a97",
   "metadata": {},
   "outputs": [],
   "source": [
    "#filter data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "29377c27",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Merge datasets\n",
    "merged_metro = pd.concat([delays_metro, onTime_metro], ignore_index=True)\n",
    "merged_metro = pd.merge(merged_metro, stops_data, left_on='stop_reference', right_on='ArRId')\n",
    "merged_metro.drop(['ArRId', 'ArRName', 'ArRType', 'transport_mode', 'recorded_at_time'], axis=1, inplace=True) \n",
    "\n",
    "merged_rer = pd.concat([delays_rer, onTime_rer], ignore_index=True)\n",
    "merged_rer = pd.merge(merged_rer, stops_data, left_on='stop_reference', right_on='ArRId')\n",
    "merged_rer.drop(['ArRId', 'ArRName', 'ArRType', 'transport_mode', 'recorded_at_time'],  axis=1, inplace=True)\n",
    "\n",
    "def date_format(df):\n",
    "    df['timestamp'] = pd.to_datetime(df['timestamp'].str.replace('Z', ''), errors='coerce')\n",
    "    df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
    "    df['real_arrival'] = pd.to_datetime(df['real_arrival']).dt.strftime('%Y-%m-%d %H:%M:%S')\n",
    "    df['real_arrival'] = pd.to_datetime(df['real_arrival'])\n",
    "    df['real_departure'] = pd.to_datetime(df['real_departure']).dt.strftime('%Y-%m-%d %H:%M:%S')\n",
    "    df['real_departure'] = pd.to_datetime(df['real_departure'])\n",
    "\n",
    "\n",
    "def format_rer(df):\n",
    "    df['scheduled_arrival'] = pd.to_datetime(df['scheduled_arrival']).dt.strftime('%Y-%m-%d %H:%M:%S')\n",
    "    df['scheduled_arrival'] = pd.to_datetime(df['scheduled_arrival'])\n",
    "    df['scheduled_departure'] = pd.to_datetime(df['scheduled_departure']).dt.strftime('%Y-%m-%d %H:%M:%S')\n",
    "    df['scheduled_departure'] = pd.to_datetime(df['scheduled_departure'])\n",
    "    df.loc[(df['arrival_difference'] >= 3) | (df['departure_difference'] >= 3), 'departure_status'] = 'delayed'\n",
    "    df['nearest_datetime'] = df['scheduled_arrival'].combine_first(df['scheduled_departure'])\n",
    "\n",
    "date_format(merged_metro)\n",
    "date_format(merged_rer)\n",
    "format_rer(merged_rer)\n",
    "\n",
    "merged_metro = merged_metro.sort_values(by='real_arrival')\n",
    "merged_rer = merged_rer.sort_values(by='nearest_datetime')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f53c7b1c",
   "metadata": {},
   "outputs": [
    {
     "ename": "IntCastingNaNError",
     "evalue": "Cannot convert non-finite values (NA or inf) to integer",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIntCastingNaNError\u001b[0m                        Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 8\u001b[0m\n\u001b[0;32m      6\u001b[0m merged_metro[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mday\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mto_datetime(merged_metro[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mreal_arrival\u001b[39m\u001b[38;5;124m'\u001b[39m])\u001b[38;5;241m.\u001b[39mdt\u001b[38;5;241m.\u001b[39mnormalize()\n\u001b[0;32m      7\u001b[0m merged_metro \u001b[38;5;241m=\u001b[39m merged_metro\u001b[38;5;241m.\u001b[39mmerge(df_with_holidays, left_on\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mday\u001b[39m\u001b[38;5;124m'\u001b[39m, right_on\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDate\u001b[39m\u001b[38;5;124m'\u001b[39m, how\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mleft\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m----> 8\u001b[0m merged_metro[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhour\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m merged_metro[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mreal_arrival\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mdt\u001b[38;5;241m.\u001b[39mhour\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mint64\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     10\u001b[0m merged_metro \u001b[38;5;241m=\u001b[39m merged_metro\u001b[38;5;241m.\u001b[39msort_values(by\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mreal_arrival\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     11\u001b[0m merged_rer \u001b[38;5;241m=\u001b[39m merged_rer\u001b[38;5;241m.\u001b[39msort_values(by\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnearest_datetime\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\graci\\anaconda3\\Lib\\site-packages\\pandas\\core\\generic.py:6643\u001b[0m, in \u001b[0;36mNDFrame.astype\u001b[1;34m(self, dtype, copy, errors)\u001b[0m\n\u001b[0;32m   6637\u001b[0m     results \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m   6638\u001b[0m         ser\u001b[38;5;241m.\u001b[39mastype(dtype, copy\u001b[38;5;241m=\u001b[39mcopy, errors\u001b[38;5;241m=\u001b[39merrors) \u001b[38;5;28;01mfor\u001b[39;00m _, ser \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitems()\n\u001b[0;32m   6639\u001b[0m     ]\n\u001b[0;32m   6641\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   6642\u001b[0m     \u001b[38;5;66;03m# else, only a single dtype is given\u001b[39;00m\n\u001b[1;32m-> 6643\u001b[0m     new_data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mgr\u001b[38;5;241m.\u001b[39mastype(dtype\u001b[38;5;241m=\u001b[39mdtype, copy\u001b[38;5;241m=\u001b[39mcopy, errors\u001b[38;5;241m=\u001b[39merrors)\n\u001b[0;32m   6644\u001b[0m     res \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_constructor_from_mgr(new_data, axes\u001b[38;5;241m=\u001b[39mnew_data\u001b[38;5;241m.\u001b[39maxes)\n\u001b[0;32m   6645\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m res\u001b[38;5;241m.\u001b[39m__finalize__(\u001b[38;5;28mself\u001b[39m, method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mastype\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\graci\\anaconda3\\Lib\\site-packages\\pandas\\core\\internals\\managers.py:430\u001b[0m, in \u001b[0;36mBaseBlockManager.astype\u001b[1;34m(self, dtype, copy, errors)\u001b[0m\n\u001b[0;32m    427\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m using_copy_on_write():\n\u001b[0;32m    428\u001b[0m     copy \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m--> 430\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply(\n\u001b[0;32m    431\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mastype\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    432\u001b[0m     dtype\u001b[38;5;241m=\u001b[39mdtype,\n\u001b[0;32m    433\u001b[0m     copy\u001b[38;5;241m=\u001b[39mcopy,\n\u001b[0;32m    434\u001b[0m     errors\u001b[38;5;241m=\u001b[39merrors,\n\u001b[0;32m    435\u001b[0m     using_cow\u001b[38;5;241m=\u001b[39musing_copy_on_write(),\n\u001b[0;32m    436\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\graci\\anaconda3\\Lib\\site-packages\\pandas\\core\\internals\\managers.py:363\u001b[0m, in \u001b[0;36mBaseBlockManager.apply\u001b[1;34m(self, f, align_keys, **kwargs)\u001b[0m\n\u001b[0;32m    361\u001b[0m         applied \u001b[38;5;241m=\u001b[39m b\u001b[38;5;241m.\u001b[39mapply(f, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    362\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 363\u001b[0m         applied \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(b, f)(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    364\u001b[0m     result_blocks \u001b[38;5;241m=\u001b[39m extend_blocks(applied, result_blocks)\n\u001b[0;32m    366\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39mfrom_blocks(result_blocks, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maxes)\n",
      "File \u001b[1;32mc:\\Users\\graci\\anaconda3\\Lib\\site-packages\\pandas\\core\\internals\\blocks.py:758\u001b[0m, in \u001b[0;36mBlock.astype\u001b[1;34m(self, dtype, copy, errors, using_cow, squeeze)\u001b[0m\n\u001b[0;32m    755\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCan not squeeze with more than one column.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    756\u001b[0m     values \u001b[38;5;241m=\u001b[39m values[\u001b[38;5;241m0\u001b[39m, :]  \u001b[38;5;66;03m# type: ignore[call-overload]\u001b[39;00m\n\u001b[1;32m--> 758\u001b[0m new_values \u001b[38;5;241m=\u001b[39m astype_array_safe(values, dtype, copy\u001b[38;5;241m=\u001b[39mcopy, errors\u001b[38;5;241m=\u001b[39merrors)\n\u001b[0;32m    760\u001b[0m new_values \u001b[38;5;241m=\u001b[39m maybe_coerce_values(new_values)\n\u001b[0;32m    762\u001b[0m refs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\graci\\anaconda3\\Lib\\site-packages\\pandas\\core\\dtypes\\astype.py:237\u001b[0m, in \u001b[0;36mastype_array_safe\u001b[1;34m(values, dtype, copy, errors)\u001b[0m\n\u001b[0;32m    234\u001b[0m     dtype \u001b[38;5;241m=\u001b[39m dtype\u001b[38;5;241m.\u001b[39mnumpy_dtype\n\u001b[0;32m    236\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 237\u001b[0m     new_values \u001b[38;5;241m=\u001b[39m astype_array(values, dtype, copy\u001b[38;5;241m=\u001b[39mcopy)\n\u001b[0;32m    238\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mValueError\u001b[39;00m, \u001b[38;5;167;01mTypeError\u001b[39;00m):\n\u001b[0;32m    239\u001b[0m     \u001b[38;5;66;03m# e.g. _astype_nansafe can fail on object-dtype of strings\u001b[39;00m\n\u001b[0;32m    240\u001b[0m     \u001b[38;5;66;03m#  trying to convert to float\u001b[39;00m\n\u001b[0;32m    241\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m errors \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\graci\\anaconda3\\Lib\\site-packages\\pandas\\core\\dtypes\\astype.py:182\u001b[0m, in \u001b[0;36mastype_array\u001b[1;34m(values, dtype, copy)\u001b[0m\n\u001b[0;32m    179\u001b[0m     values \u001b[38;5;241m=\u001b[39m values\u001b[38;5;241m.\u001b[39mastype(dtype, copy\u001b[38;5;241m=\u001b[39mcopy)\n\u001b[0;32m    181\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 182\u001b[0m     values \u001b[38;5;241m=\u001b[39m _astype_nansafe(values, dtype, copy\u001b[38;5;241m=\u001b[39mcopy)\n\u001b[0;32m    184\u001b[0m \u001b[38;5;66;03m# in pandas we don't store numpy str dtypes, so convert to object\u001b[39;00m\n\u001b[0;32m    185\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(dtype, np\u001b[38;5;241m.\u001b[39mdtype) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28missubclass\u001b[39m(values\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;241m.\u001b[39mtype, \u001b[38;5;28mstr\u001b[39m):\n",
      "File \u001b[1;32mc:\\Users\\graci\\anaconda3\\Lib\\site-packages\\pandas\\core\\dtypes\\astype.py:101\u001b[0m, in \u001b[0;36m_astype_nansafe\u001b[1;34m(arr, dtype, copy, skipna)\u001b[0m\n\u001b[0;32m     96\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m lib\u001b[38;5;241m.\u001b[39mensure_string_array(\n\u001b[0;32m     97\u001b[0m         arr, skipna\u001b[38;5;241m=\u001b[39mskipna, convert_na_value\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m     98\u001b[0m     )\u001b[38;5;241m.\u001b[39mreshape(shape)\n\u001b[0;32m    100\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m np\u001b[38;5;241m.\u001b[39missubdtype(arr\u001b[38;5;241m.\u001b[39mdtype, np\u001b[38;5;241m.\u001b[39mfloating) \u001b[38;5;129;01mand\u001b[39;00m dtype\u001b[38;5;241m.\u001b[39mkind \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124miu\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 101\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _astype_float_to_int_nansafe(arr, dtype, copy)\n\u001b[0;32m    103\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m arr\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mobject\u001b[39m:\n\u001b[0;32m    104\u001b[0m     \u001b[38;5;66;03m# if we have a datetime/timedelta array of objects\u001b[39;00m\n\u001b[0;32m    105\u001b[0m     \u001b[38;5;66;03m# then coerce to datetime64[ns] and use DatetimeArray.astype\u001b[39;00m\n\u001b[0;32m    107\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m lib\u001b[38;5;241m.\u001b[39mis_np_dtype(dtype, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mM\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[1;32mc:\\Users\\graci\\anaconda3\\Lib\\site-packages\\pandas\\core\\dtypes\\astype.py:145\u001b[0m, in \u001b[0;36m_astype_float_to_int_nansafe\u001b[1;34m(values, dtype, copy)\u001b[0m\n\u001b[0;32m    141\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    142\u001b[0m \u001b[38;5;124;03mastype with a check preventing converting NaN to an meaningless integer value.\u001b[39;00m\n\u001b[0;32m    143\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    144\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m np\u001b[38;5;241m.\u001b[39misfinite(values)\u001b[38;5;241m.\u001b[39mall():\n\u001b[1;32m--> 145\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m IntCastingNaNError(\n\u001b[0;32m    146\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot convert non-finite values (NA or inf) to integer\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    147\u001b[0m     )\n\u001b[0;32m    148\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dtype\u001b[38;5;241m.\u001b[39mkind \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mu\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    149\u001b[0m     \u001b[38;5;66;03m# GH#45151\u001b[39;00m\n\u001b[0;32m    150\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (values \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mall():\n",
      "\u001b[1;31mIntCastingNaNError\u001b[0m: Cannot convert non-finite values (NA or inf) to integer"
     ]
    }
   ],
   "source": [
    "#Add holidays\n",
    "merged_rer['day'] = pd.to_datetime(merged_rer['nearest_datetime']).dt.normalize()\n",
    "merged_rer = merged_rer.merge(df_with_holidays, left_on='day', right_on='Date', how='left')\n",
    "merged_rer['hour'] = merged_rer['nearest_datetime'].dt.hour.astype('int64')\n",
    "\n",
    "merged_metro['day'] = pd.to_datetime(merged_metro['real_arrival']).dt.normalize()\n",
    "merged_metro = merged_metro.merge(df_with_holidays, left_on='day', right_on='Date', how='left')\n",
    "merged_metro['hour'] = merged_metro['real_arrival'].dt.hour.astype('int64')\n",
    "\n",
    "merged_metro = merged_metro.sort_values(by='real_arrival')\n",
    "merged_rer = merged_rer.sort_values(by='nearest_datetime')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df9a80a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_metro = merged_metro.merge(trafic2023_ratio_metro, left_on=['day_type', 'hour'], right_on=['CAT_JOUR', 'start_hour'], how='left')\n",
    "#SGP = SGP.merge(SGP_traffic, left_on=['day_type', 'hour'], right_on=['CAT_JOUR', 'start_hour'], how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "id": "03137ccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_rer['scheduled_arrival'] = pd.to_datetime(merged_rer['scheduled_arrival'], format='%Y-%m-%d %H:%M:%S')\n",
    "merged_rer['pourc_validations'] = merged_rer['pourc_validations'].fillna(0)\n",
    "merged_rer[\"departure_status\"] = merged_rer[\"departure_status\"].astype(\"string\")\n",
    "\n",
    "merged_metro['real_arrival'] = pd.to_datetime(merged_metro['real_arrival'], format='%Y-%m-%d %H:%M:%S')\n",
    "merged_metro['pourc_validations'] = merged_metro['pourc_validations'].fillna(0)\n",
    "merged_metro[\"departure_status\"] = merged_metro[\"departure_status\"].astype(\"string\")\n",
    "\n",
    "start_date = pd.to_datetime('2024-11-02 00:00:00')\n",
    "\n",
    "merged_metro = merged_metro[(merged_metro['real_arrival'] >= start_date)]\n",
    "merged_rer = merged_rer[(merged_rer['scheduled_arrival'] >= start_date)]"
   ]
  },
  {
   "cell_type": "raw",
   "id": "cbca9d9b",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "#Clean Gas Price data\n",
    "def clean_gas_data(file_path, fuel_type):\n",
    "    df = pd.read_csv(file_path, skiprows=3, sep=';')\n",
    "    df.rename(columns={df.columns[1]: f'{fuel_type}'}, inplace=True)\n",
    "    df = df.iloc[:, :-1]\n",
    "    return df\n",
    "    \n",
    "gas_95_df = clean_gas_data(\"./Online_Data/Gas/octane_95.csv\", \"95\")\n",
    "gas_98_df = clean_gas_data(\"./Online_Data/Gas/octane_98.csv\", \"98\")\n",
    "gas_e10_df = clean_gas_data(\"./Online_Data/Gas/95-E10.csv\", \"E10\")\n",
    "gazole_df = clean_gas_data(\"./Online_Data/Gas/gazole.csv\", \"gazole\")\n",
    "\n",
    "merged_rer['Période'] = pd.to_datetime(merged_rer['nearest_datetime']).dt.strftime('%Y-%m')\n",
    "merged_rer = merged_rer.merge(gas_95_df, on='Période', how='left')\n",
    "merged_rer = merged_rer.merge(gas_98_df, on='Période', how='left')\n",
    "merged_rer = merged_rer.merge(gas_e10_df, on='Période', how='left')\n",
    "merged_rer = merged_rer.merge(gazole_df, on='Période', how='left')\n",
    "\n",
    "merged_metro.drop(columns=['timestamp'], inplace=True)\n",
    "#merged_rer.drop(columns=['Période', 'timestamp'], inplace=True)\n",
    "merged_rer = merged_rer[~merged_rer.duplicated(subset=merged_rer.columns.difference(['delayed_status']).tolist(), keep='last')]\n",
    "\n",
    "merged_rer.loc[merged_rer['scheduled_arrival'].dt.month == 1, 'E10'] = 1.76\n",
    "merged_rer.loc[merged_rer['scheduled_arrival'].dt.month == 1, '95'] = 1.79\n",
    "merged_rer.loc[merged_rer['scheduled_arrival'].dt.month == 1, '98'] = 1.87\n",
    "merged_rer.loc[merged_rer['scheduled_arrival'].dt.month == 1, 'gazole'] = 1.70"
   ]
  },
  {
   "cell_type": "raw",
   "id": "2ec9b15e",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "merged_metro = merged_metro.dropna(subset=['real_arrival'])\n",
    "merged_rer= merged_rer.dropna(subset=['nearest_datetime'])\n",
    "# I drop missing value to be able to macth with weather"
   ]
  },
  {
   "cell_type": "raw",
   "id": "189c8560",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "#Weather Variable\n",
    "\n",
    "#hourly_weather['date'] = pd.to_datetime(hourly_weather['date']) - don't need hourly?\n",
    "minutely_15_weather['date'] = pd.to_datetime(minutely_15_weather['date'])\n",
    "\n",
    "merged_metro = pd.merge_asof(merged_metro, \n",
    "                   minutely_15_weather, \n",
    "                   left_on='real_arrival', \n",
    "                   right_on='date', \n",
    "                   direction='nearest')\n",
    "\n",
    "merged_rer = pd.merge_asof(merged_rer, \n",
    "                   minutely_15_weather, \n",
    "                   left_on='nearest_datetime', \n",
    "                   right_on='date', \n",
    "                   direction='nearest')"
   ]
  },
  {
   "cell_type": "raw",
   "id": "b82074a5-aa74-497f-9fcb-a251cb9335d1",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "APK = pd.read_csv(\"./Stations/APK.csv\")\n",
    "GBG = pd.read_csv(\"./Stations/GBG.csv\")\n",
    "CLH = pd.read_csv(\"./Stations/CLH.csv\", low_memory=False)\n",
    "MP = pd.read_csv(\"./Stations/MP.csv\")\n",
    "\n",
    "rer = pd.concat([APK, GBG, CLH, MP], ignore_index=True)\n",
    "\n",
    "CDG = pd.read_csv(\"./Stations/CDG.csv\")\n",
    "SL = pd.read_csv(\"./Stations/SL.csv\")\n",
    "SGP = pd.read_csv(\"./Stations/SGP.csv\")\n",
    "BL = pd.read_csv(\"./Stations/BL.csv\")\n",
    "\n",
    "metro = pd.concat([CDG, SL, SGP, BL], ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3213bead-d10e-48fc-98b0-5ef10709af3c",
   "metadata": {},
   "source": [
    "# Summary Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f588414d-2559-4f67-9301-78c17e552c1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## for better readability, merge noth/south east/west directions\n",
    "\n",
    "# Ensure the datetime column is in the correct format (if not already)\n",
    "CLH['scheduled_arrival'] = pd.to_datetime(CLH['scheduled_arrival'])\n",
    "\n",
    "# Filter for delayed departures\n",
    "delayed_rer = CLH[CLH['departure_status'] == 'delayed']\n",
    "\n",
    "# Add a 'day_of_week' column (0 = Monday, 1 = Tuesday, ..., 6 = Sunday) using .loc\n",
    "delayed_rer.loc[:, 'day_of_week'] = delayed_rer['scheduled_arrival'].dt.dayofweek\n",
    "\n",
    "# Map numerical days to actual weekday names using .loc\n",
    "day_names = {0: 'Monday', 1: 'Tuesday', 2: 'Wednesday', 3: 'Thursday', 4: 'Friday', 5: 'Saturday', 6: 'Sunday'}\n",
    "delayed_rer.loc[:, 'day_of_week'] = delayed_rer['day_of_week'].map(day_names)\n",
    "\n",
    "# Group by day of the week and destination_name, then count the number of delays\n",
    "delayed_count_per_weekday = delayed_rer.groupby(['day_of_week', 'destination_name']).size().reset_index(name='delay_count')\n",
    "\n",
    "# Calculate the average number of delays per weekday for each destination\n",
    "average_delays_per_weekday = delayed_count_per_weekday.groupby(['day_of_week', 'destination_name'])['delay_count'].mean().reset_index()\n",
    "\n",
    "# Sort by weekday order\n",
    "ordered_days = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']\n",
    "average_delays_per_weekday['day_of_week'] = pd.Categorical(average_delays_per_weekday['day_of_week'], categories=ordered_days, ordered=True)\n",
    "\n",
    "# Plot the average delays per weekday for each destination as a bar chart\n",
    "plt.figure(figsize=(14, 6))\n",
    "sns.barplot(\n",
    "    data=average_delays_per_weekday,\n",
    "    x='day_of_week',\n",
    "    y='delay_count',\n",
    "    hue='destination_name',\n",
    "    palette = \"tab10\"\n",
    ")\n",
    "\n",
    "# Add labels and title\n",
    "plt.xlabel('Day of Week')\n",
    "plt.ylabel('Average Number of Delays')\n",
    "plt.title('Average Number of Delays Per Weekday by Destination')\n",
    "\n",
    "# Rotate x-ticks for better readability\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "# Add grid for better readability\n",
    "plt.grid(True)\n",
    "\n",
    "# Show the plot\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae5ec651-1f5e-4b4f-a8de-97c10e322301",
   "metadata": {},
   "outputs": [],
   "source": [
    "rer[\"stop_name\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e10f4e59-e4cc-4323-a49b-429b42c46d5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure datetime format\n",
    "rer_filtered = rer[rer['arrival_difference'] <= 100].copy()\n",
    "rer_filtered['hour'] = rer_filtered['scheduled_arrival'].dt.hour\n",
    "\n",
    "# Group by 'hour' and 'ArRTown' to calculate mean delay\n",
    "average_delay_by_category = rer_filtered.groupby(['hour', 'stop_name'])['arrival_difference'].mean().reset_index()\n",
    "\n",
    "custom_palette = {\n",
    "    'Massy - Palaiseau': '#4B92DB',\n",
    "    'Avenue du Président Kennedy': '#F3D311',\n",
    "    'Châtelet - Les Halles': '#F7403A',\n",
    "    'Gare de Buno Gironville': '#3F9C35'\n",
    "}\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.lineplot(\n",
    "    data=average_delay_by_category,\n",
    "    x='hour',\n",
    "    y='arrival_difference',\n",
    "    hue='stop_name',\n",
    "    palette=custom_palette,\n",
    "    marker=\"o\"\n",
    ")\n",
    "\n",
    "#plt.xticks(ticks=range(0, 24), labels=[str(hour) for hour in range(24)])\n",
    "plt.gca().set_xticks(range(0, 24, 4))  # Major ticks every 4 hours\n",
    "\n",
    "plt.xlabel('Hour of the Day')\n",
    "plt.ylabel('Average Delay (in mins)')\n",
    "plt.title('Average Delay Duration Over a Single Day by Stop')\n",
    "plt.legend(title=\"stop_name\")\n",
    "plt.grid(True)\n",
    "plt.savefig(\"./Figures/Average-Delay-Stop.png\", format='png', dpi=300)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1255ed96-fa39-4a81-b714-38b23630ef18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove outliers where delay exceeds 100 seconds\n",
    "rer_filtered = rer[rer['arrival_difference'] <= 100]\n",
    "\n",
    "rer_filtered.loc[:, 'day_of_week'] = rer_filtered['scheduled_arrival'].dt.dayofweek\n",
    "rer_filtered.loc[:, 'day_of_week'] = rer_filtered['day_of_week'].map(day_names)\n",
    "rer_filtered.loc[:, 'day_of_week'] = pd.Categorical(\n",
    "    rer_filtered['day_of_week'],\n",
    "    categories=['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday'],\n",
    "    ordered=True\n",
    ")\n",
    "\n",
    "# Group by 'day_of_week' and 'ArRTown' to calculate mean delay\n",
    "average_delay_by_day = rer_filtered.groupby(\n",
    "    ['day_of_week', 'stop_name'], observed=False\n",
    ")['arrival_difference'].mean().reset_index()\n",
    "\n",
    "\n",
    "custom_palette = {\n",
    "    'Massy - Palaiseau': '#4B92DB',\n",
    "    'Avenue du Président Kennedy': '#F3D311',\n",
    "    'Châtelet - Les Halles': '#F7403A',\n",
    "    'Gare de Buno Gironville': '#3F9C35'\n",
    "}\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.lineplot(\n",
    "    data=average_delay_by_day,\n",
    "    x='day_of_week',\n",
    "    y='arrival_difference',\n",
    "    hue='stop_name',\n",
    "    palette=custom_palette,\n",
    "    marker=\"o\"\n",
    ")\n",
    "plt.xlabel('Day of the Week')\n",
    "plt.ylabel('Average Delay (in secs)')\n",
    "plt.title('Average Delay Duration by Day of the Week and Stop (Filtered)')\n",
    "plt.legend(title=\"stop_name\")\n",
    "plt.grid(True)\n",
    "plt.savefig(\"./Figures/Average-Delay-DayOfWeek-Stop.png\", format='png', dpi=300)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b93da86-3f34-4d27-b2ec-df0dd7ed7381",
   "metadata": {},
   "outputs": [],
   "source": [
    "## idk how to format the dates propertly and on axis ticker location\n",
    "\n",
    "# Ensure the datetime column is in proper format\n",
    "rer['Date'] = pd.to_datetime(rer['Date'])\n",
    "\n",
    "# Filter for delayed departures\n",
    "#delayed_rer = rer[rer['departure_status'] == 'delayed']\n",
    "\n",
    "# Group by date and count the number of delays\n",
    "#delayed_count_per_day = delayed_rer.groupby(delayed_rer['Date'].dt.date).size().reset_index(name='delay_count')\n",
    "average_delay_by_category = rer_filtered.groupby(['Date', 'stop_name'])['arrival_difference'].mean().reset_index()\n",
    "\n",
    "# Rename columns for clarity\n",
    "average_delay_by_category.rename(columns={'Date': 'date'}, inplace=True)\n",
    "\n",
    "# Convert 'date' to datetime for better handling of date ticks\n",
    "average_delay_by_category['date'] = pd.to_datetime(average_delay_by_category['date'])\n",
    "\n",
    "# Plot as a bar chart\n",
    "plt.figure(figsize=(14, 6))\n",
    "sns.barplot(\n",
    "    data=average_delay_by_category,\n",
    "    x='date',\n",
    "    y='arrival_difference',\n",
    "    color=\"#FF4500\"  # Custom orange-red color\n",
    ")\n",
    "\n",
    "# Set x-axis ticks to be on each Monday\n",
    "plt.gca().xaxis.set_major_locator(mdates.WeekdayLocator(byweekday=mdates.MO))  # Major ticks every Monday\n",
    "\n",
    "# Get the current axis and set the ticks\n",
    "ax = plt.gca()\n",
    "ticks = ax.get_xticks()\n",
    "\n",
    "# Alternate the height of the ticks\n",
    "for i, tick in enumerate(ticks):\n",
    "    if i % 2 == 0:\n",
    "        ax.get_xticklabels()[i].set_verticalalignment('bottom')  # Lower even-index ticks\n",
    "    else:\n",
    "        ax.get_xticklabels()[i].set_verticalalignment('top')  # Raise odd-index ticks\n",
    "\n",
    "# Add labels and title\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Duration of Delays')\n",
    "plt.title('Duration of Delays Per Day')\n",
    "\n",
    "# Enable grid for better visibility of bar heights\n",
    "plt.grid(axis='y', linestyle='--', linewidth=0.7)\n",
    "\n",
    "# Tight layout for better spacing\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save and show the plot\n",
    "plt.savefig(\"./Figures/Duration-Delays-Per-Day-Weekly-Ticks-Alternating.png\", format='png', dpi=300)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2f4d1ca-2cc4-473a-8554-8dbb8430b775",
   "metadata": {},
   "outputs": [],
   "source": [
    "## idk how to format the dates propertly and on axis ticker location\n",
    "\n",
    "# Ensure the datetime column is in proper format\n",
    "rer['Date'] = pd.to_datetime(rer['Date'])\n",
    "\n",
    "# Filter for delayed departures\n",
    "delayed_rer = rer[rer['departure_status'] == 'delayed']\n",
    "\n",
    "# Group by date and count the number of delays\n",
    "delayed_count_per_day = delayed_rer.groupby(delayed_rer['Date'].dt.date).size().reset_index(name='delay_count')\n",
    "\n",
    "# Rename columns for clarity\n",
    "delayed_count_per_day.rename(columns={'Date': 'date'}, inplace=True)\n",
    "\n",
    "# Convert 'date' to datetime for better handling of date ticks\n",
    "delayed_count_per_day['date'] = pd.to_datetime(delayed_count_per_day['date'])\n",
    "\n",
    "# Plot as a bar chart\n",
    "plt.figure(figsize=(14, 6))\n",
    "sns.barplot(\n",
    "    data=delayed_count_per_day,\n",
    "    x='date',\n",
    "    y='delay_count',\n",
    "    color=\"#FF4500\"  # Custom orange-red color\n",
    ")\n",
    "\n",
    "# Set x-axis ticks to be on each Monday\n",
    "plt.gca().xaxis.set_major_locator(mdates.WeekdayLocator(byweekday=mdates.MO))  # Major ticks every Monday\n",
    "\n",
    "# Get the current axis and set the ticks\n",
    "ax = plt.gca()\n",
    "ticks = ax.get_xticks()\n",
    "\n",
    "# Alternate the height of the ticks\n",
    "for i, tick in enumerate(ticks):\n",
    "    if i % 2 == 0:\n",
    "        ax.get_xticklabels()[i].set_verticalalignment('bottom')  # Lower even-index ticks\n",
    "    else:\n",
    "        ax.get_xticklabels()[i].set_verticalalignment('top')  # Raise odd-index ticks\n",
    "\n",
    "# Add labels and title\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Number of Delays')\n",
    "plt.title('Number of Delays Per Day')\n",
    "\n",
    "# Enable grid for better visibility of bar heights\n",
    "plt.grid(axis='y', linestyle='--', linewidth=0.7)\n",
    "\n",
    "# Tight layout for better spacing\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save and show the plot\n",
    "plt.savefig(\"./Figures/Delays-Per-Day-Weekly-Ticks-Alternating.png\", format='png', dpi=300)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62c0abd0-83a2-4631-9efa-9f2d7197d996",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select numerical variables\n",
    "numerical_vars = metro_learn.select_dtypes(include=['float64'])\n",
    "\n",
    "# Standardize the numerical variables\n",
    "scaler = StandardScaler()\n",
    "numerical_vars_scaled = scaler.fit_transform(numerical_vars)\n",
    "\n",
    "# Perform PCA\n",
    "pca = PCA()\n",
    "pca_results = pca.fit_transform(numerical_vars_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "031213ec-7dc1-4861-9791-7a9f334a7019",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame for the PCA results\n",
    "pca_df = pd.DataFrame(\n",
    "    pca_results,\n",
    "    columns=[f'PC{i+1}' for i in range(pca_results.shape[1])]\n",
    ")\n",
    "\n",
    "# Explained variance ratio\n",
    "explained_variance = pca.explained_variance_ratio_\n",
    "\n",
    "# Output the PCA results and explained variance\n",
    "print(\"Explained Variance Ratio:\", explained_variance)\n",
    "print(\"PCA DataFrame Head:\\n\", pca_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d72dffbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -or\n",
    "# Assuming you already have your dataset loaded into 'APK' DataFrame and it's preprocessed\n",
    "\n",
    "# Select numerical columns\n",
    "numerical_vars = metro_learn.select_dtypes(include=['float64'])\n",
    "\n",
    "# Standardize the data\n",
    "scaler = StandardScaler()\n",
    "numerical_vars_scaled = scaler.fit_transform(numerical_vars)\n",
    "\n",
    "# Perform PCA\n",
    "pca = PCA()\n",
    "pca.fit(numerical_vars_scaled)\n",
    "\n",
    "# Eigenvalues (explained variance for each principal component)\n",
    "eigval = pca.explained_variance_\n",
    "\n",
    "# Number of components (k)\n",
    "k = len(eigval)\n",
    "\n",
    "# Scree plot\n",
    "plt.plot(np.arange(1, k + 1), eigval, marker='o')\n",
    "plt.title(\"Scree plot\")\n",
    "plt.ylabel(\"Eigen values\")\n",
    "plt.xlabel(\"Factor number\")\n",
    "plt.grid(True)\n",
    "plt.savefig('Screeplot.png', bbox_inches='tight', dpi=300)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbc4fe7e-8840-433d-b144-56882ef68e38",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.arange(1,k+1),np.cumsum(pca.explained_variance_ratio_)) \n",
    "plt.title(\"Explained variance vs. # of factors\") \n",
    "plt.ylabel(\"Cumsum explained variance ratio\") \n",
    "plt.xlabel(\"Factor number\") \n",
    "plt.savefig('CumSumPlot.png', bbox_inches='tight', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "id": "d7b2c105-1fda-4939-84c8-76d5ac14ec68",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 5 factors picked for expainability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63af2871-144a-450f-b8e5-aa3b4fd46b36",
   "metadata": {},
   "outputs": [],
   "source": [
    "eigval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "654ebc3d-25be-4c79-b5d1-3254b391b15f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.cumsum(pca.explained_variance_ratio_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89b298b9-cc25-465f-8c09-15e4437a4444",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming APK_learn is your DataFrame, and categorical columns are encoded as integers\n",
    "# Select columns that are of integer type (numerical encoded categorical columns)\n",
    "categorical_columns = APK_learn.select_dtypes(include=['int64']).columns\n",
    "\n",
    "# Run MCA using the mca library, specifying n_components for multiple components\n",
    "mca_model = mca.MCA(APK_learn[categorical_columns], ncols=7)  # Request 7 components\n",
    "\n",
    "# Access the MCA results (coordinates of the data points in the reduced space)\n",
    "mca_results = mca_model.fs_r()  # Call the function to get the result\n",
    "\n",
    "# Create a DataFrame with the MCA results\n",
    "mca_df = pd.DataFrame(mca_results, columns=[f'MCA{i+1}' for i in range(mca_results.shape[1])])\n",
    "\n",
    "# Display the first few rows of the MCA results\n",
    "print(mca_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e42c08d-9007-4570-bac4-f48a176ec9c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optionally, you can plot the first two components (MCA1 vs MCA2)\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(mca_df['MCA1'], mca_df['MCA2'], alpha=0.5)\n",
    "plt.title('MCA: First Two Components')\n",
    "plt.xlabel('MCA1')\n",
    "plt.ylabel('MCA2')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "3a53a886-b6ec-43a7-8a38-c2ad6c456998",
   "metadata": {},
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Assuming APK_learn is your DataFrame and 'departure_status' is your target column\n",
    "\n",
    "# Step 1: Separate the features and target variable\n",
    "X = APK_learn.drop(columns=['departure_status'])  # All columns except the target\n",
    "y = APK_learn['departure_status']  # Target variable\n",
    "\n",
    "# Step 2: Identify numerical and categorical columns\n",
    "numerical_columns = X.select_dtypes(include=['float64']).columns\n",
    "categorical_columns = X.select_dtypes(include=['int64']).columns\n",
    "\n",
    "# Step 3: Preprocessing for numerical columns\n",
    "# Impute missing values and standardize\n",
    "numerical_pipeline = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='mean')),  # Impute missing values with the mean\n",
    "    ('scaler', StandardScaler())  # Standardize numerical columns\n",
    "])\n",
    "\n",
    "# Step 4: Preprocessing for categorical columns\n",
    "# Impute missing values and one-hot encode categorical columns\n",
    "categorical_pipeline = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),  # Impute missing values with the most frequent value\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))  # One-hot encode categorical columns\n",
    "])\n",
    "\n",
    "# Step 5: Combine both preprocessing pipelines using ColumnTransformer\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numerical_pipeline, numerical_columns),\n",
    "        ('cat', categorical_pipeline, categorical_columns)\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Step 6: Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Step 7: Fit and transform the X_train data using the preprocessor\n",
    "X_train_transformed = preprocessor.fit_transform(X_train)\n",
    "\n",
    "# Step 8: Define the Random Forest Classifier and the parameter grid for GridSearchCV\n",
    "rf = RandomForestClassifier(random_state=42)\n",
    "\n",
    "# Define the parameter grid for GridSearchCV\n",
    "rf_grid = {\n",
    "    'n_estimators': [50, 100, 150],\n",
    "    'max_depth': [5, 10, 15, None],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4]\n",
    "}\n",
    "\n",
    "# Step 9: Perform GridSearchCV with StratifiedKFold cross-validation\n",
    "cv_folds = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n",
    "rf_cv = GridSearchCV(rf, rf_grid, cv=cv_folds, n_jobs=-1, scoring='accuracy')\n",
    "\n",
    "# Step 10: Fit the Random Forest model using GridSearchCV\n",
    "rf_cv.fit(X_train_transformed, y_train)\n",
    "\n",
    "# Step 11: Output the best hyperparameters found by GridSearchCV\n",
    "print(f\"Best hyperparameters: {rf_cv.best_params_}\")\n",
    "\n",
    "# Step 12: Transform the X_test data using the preprocessor and make predictions on the test set\n",
    "X_test_transformed = preprocessor.transform(X_test)\n",
    "y_pred = rf_cv.predict(X_test_transformed)\n",
    "\n",
    "# Step 13: Evaluate the model\n",
    "print(f\"Accuracy: {accuracy_score(y_test, y_pred):.4f}\")\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "1f40aece-e0fb-4d50-bb59-58f57dd3fc9e",
   "metadata": {},
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# Assuming APK_learn is your DataFrame and 'departure_status' is your target column\n",
    "\n",
    "# Step 1: Separate the features and target variable\n",
    "X = APK_learn.drop(columns=['departure_status'])  # All columns except the target\n",
    "y = APK_learn['departure_status']  # Target variable\n",
    "\n",
    "# Step 2: Identify numerical and categorical columns\n",
    "numerical_columns = X.select_dtypes(include=['float64']).columns\n",
    "categorical_columns = X.select_dtypes(include=['int64']).columns\n",
    "\n",
    "# Step 3: Preprocessing for numerical columns\n",
    "# Impute missing values and standardize\n",
    "numerical_pipeline = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='mean')),  # Impute missing values with the mean\n",
    "    ('scaler', StandardScaler())  # Standardize numerical columns\n",
    "])\n",
    "\n",
    "# Step 4: Preprocessing for categorical columns\n",
    "# Impute missing values and one-hot encode categorical columns\n",
    "categorical_pipeline = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),  # Impute missing values with the most frequent value\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))  # One-hot encode categorical columns\n",
    "])\n",
    "\n",
    "# Step 5: Combine both preprocessing pipelines using ColumnTransformer\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numerical_pipeline, numerical_columns),\n",
    "        ('cat', categorical_pipeline, categorical_columns)\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Step 6: Create the Random Forest Classifier pipeline (without setting hyperparameters yet)\n",
    "rf_pipeline = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', RandomForestClassifier(random_state=0))  # No hyperparameters set here yet\n",
    "])\n",
    "\n",
    "# Step 7: Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Step 8: Define the parameter grid for GridSearchCV\n",
    "rf_grid = {\n",
    "    'classifier__n_estimators': [50, 100, 150],\n",
    "    'classifier__max_depth': [5, 10, 15, None],\n",
    "    'classifier__min_samples_split': [2, 5, 10],\n",
    "    'classifier__min_samples_leaf': [1, 2, 4]\n",
    "}\n",
    "\n",
    "# Step 9: Perform GridSearchCV with StratifiedKFold cross-validation\n",
    "cv_folds = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n",
    "rf_cv = GridSearchCV(rf_pipeline, rf_grid, cv=cv_folds, n_jobs=-1, scoring='accuracy')\n",
    "\n",
    "# Step 10: Fit the model using GridSearchCV\n",
    "rf_cv.fit(X_train, y_train)\n",
    "\n",
    "# Step 11: Output the best hyperparameters found by GridSearchCV\n",
    "print(f\"Best hyperparameters: {rf_cv.best_params_}\")\n",
    "\n",
    "# Step 12: Make predictions on the test set\n",
    "y_pred = rf_cv.predict(X_test)\n",
    "\n",
    "# Step 13: Evaluate the model\n",
    "print(f\"Accuracy: {accuracy_score(y_test, y_pred):.4f}\")\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2d40cee-5c87-441f-a927-38b38c8d0b46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure the datetime column is in the correct format (if not already)\n",
    "rer['scheduled_arrival'] = pd.to_datetime(rer['scheduled_arrival'])\n",
    "\n",
    "# Extract hour from the datetime column\n",
    "rer['hour'] = rer['scheduled_arrival'].dt.hour\n",
    "\n",
    "# Group by hour and calculate the average pourc_validations\n",
    "hourly_validations = rer.groupby('hour')['pourc_validations'].mean().reset_index()\n",
    "\n",
    "# Plot the hourly average pourc_validations\n",
    "plt.figure(figsize=(14, 6))\n",
    "sns.lineplot(\n",
    "    data=hourly_validations,\n",
    "    x='hour',\n",
    "    y='pourc_validations',\n",
    "    marker='o',\n",
    "    color='b'  # Blue color for the line\n",
    ")\n",
    "\n",
    "# Add labels and title\n",
    "plt.xlabel('Hour of Day')\n",
    "plt.ylabel('Average Pourc Validations')\n",
    "plt.title('Average Pourc Validations Per Hour of Day')\n",
    "\n",
    "# Set the x-ticks to be from 0 to 23 (representing hours)\n",
    "plt.xticks(range(24))\n",
    "\n",
    "# Add grid for better readability\n",
    "plt.grid(True)\n",
    "\n",
    "# Show the plot\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41c95e8e-385e-406c-b99d-a49a416ebc12",
   "metadata": {},
   "source": [
    "# metro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8418d142-de9f-431f-a7fb-ac007f330414",
   "metadata": {},
   "outputs": [],
   "source": [
    "## for better readability, merge noth/south east/west directions\n",
    "\n",
    "# Ensure the datetime column is in the correct format (if not already)\n",
    "metro['real_arrival'] = pd.to_datetime(metro['real_arrival'])\n",
    "\n",
    "# Filter for delayed departures\n",
    "delayed_rer = metro[metro['departure_status'] == 'delayed']\n",
    "\n",
    "# Add a 'day_of_week' column (0 = Monday, 1 = Tuesday, ..., 6 = Sunday) using .loc\n",
    "delayed_rer.loc[:, 'day_of_week'] = delayed_rer['real_arrival'].dt.dayofweek\n",
    "\n",
    "# Map numerical days to actual weekday names using .loc\n",
    "day_names = {0: 'Monday', 1: 'Tuesday', 2: 'Wednesday', 3: 'Thursday', 4: 'Friday', 5: 'Saturday', 6: 'Sunday'}\n",
    "delayed_rer.loc[:, 'day_of_week'] = delayed_rer['day_of_week'].map(day_names)\n",
    "\n",
    "# Group by day of the week and destination_name, then count the number of delays\n",
    "delayed_count_per_weekday = delayed_rer.groupby(['day_of_week', 'destination_name']).size().reset_index(name='delay_count')\n",
    "\n",
    "# Calculate the average number of delays per weekday for each destination\n",
    "average_delays_per_weekday = delayed_count_per_weekday.groupby(['day_of_week', 'destination_name'])['delay_count'].mean().reset_index()\n",
    "\n",
    "# Sort by weekday order\n",
    "ordered_days = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']\n",
    "average_delays_per_weekday['day_of_week'] = pd.Categorical(average_delays_per_weekday['day_of_week'], categories=ordered_days, ordered=True)\n",
    "\n",
    "# Plot the average delays per weekday for each destination as a bar chart\n",
    "plt.figure(figsize=(14, 6))\n",
    "sns.barplot(\n",
    "    data=average_delays_per_weekday,\n",
    "    x='day_of_week',\n",
    "    y='delay_count',\n",
    "    hue='destination_name',\n",
    "    palette = \"tab10\"\n",
    ")\n",
    "\n",
    "# Add labels and title\n",
    "plt.xlabel('Day of Week')\n",
    "plt.ylabel('Average Number of Delays')\n",
    "plt.title('Average Number of Delays Per Weekday by Destination')\n",
    "\n",
    "# Rotate x-ticks for better readability\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "# Add grid for better readability\n",
    "plt.grid(True)\n",
    "\n",
    "# Show the plot\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bcd4ca5-d6c6-4f6c-9f39-f4ab18d3fb7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## idk how to format the dates propertly and on axis ticker location\n",
    "\n",
    "# Ensure the datetime column is in proper format\n",
    "metro['real_arrival'] = pd.to_datetime(metro['real_arrival'])\n",
    "\n",
    "# Filter for delayed departures\n",
    "delayed_metro = metro[metro['departure_status'] == 'delayed']\n",
    "\n",
    "# Group by date and count the number of delays\n",
    "delayed_count_per_day = delayed_metro.groupby(delayed_metro['real_arrival'].dt.date).size().reset_index(name='delay_count')\n",
    "\n",
    "# Rename columns for clarity\n",
    "delayed_count_per_day.rename(columns={'real_arrival': 'date'}, inplace=True)\n",
    "\n",
    "# Convert 'date' to datetime for better handling of date ticks\n",
    "delayed_count_per_day['date'] = pd.to_datetime(delayed_count_per_day['date'])\n",
    "\n",
    "# Plot as a bar chart\n",
    "plt.figure(figsize=(14, 6))\n",
    "sns.barplot(\n",
    "    data=delayed_count_per_day,\n",
    "    x='date',\n",
    "    y='delay_count',\n",
    "    color=\"#FF4500\"  # Custom orange-red color\n",
    ")\n",
    "\n",
    "# Set x-axis ticks to be on each Monday\n",
    "plt.gca().xaxis.set_major_locator(mdates.WeekdayLocator(byweekday=mdates.MO))  # Major ticks every Monday\n",
    "\n",
    "# Get the current axis and set the ticks\n",
    "ax = plt.gca()\n",
    "ticks = ax.get_xticks()\n",
    "\n",
    "# Alternate the height of the ticks\n",
    "for i, tick in enumerate(ticks):\n",
    "    if i % 2 == 0:\n",
    "        ax.get_xticklabels()[i].set_verticalalignment('bottom')  # Lower even-index ticks\n",
    "    else:\n",
    "        ax.get_xticklabels()[i].set_verticalalignment('top')  # Raise odd-index ticks\n",
    "\n",
    "# Add labels and title\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Number of Delays')\n",
    "plt.title('Number of Delays Per Day')\n",
    "\n",
    "# Enable grid for better visibility of bar heights\n",
    "plt.grid(axis='y', linestyle='--', linewidth=0.7)\n",
    "\n",
    "# Tight layout for better spacing\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save and show the plot\n",
    "plt.savefig(\"./Figures/Delays-Per-Day-Weekly-Ticks-Alternating.png\", format='png', dpi=300)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d928482-2a73-4048-a088-5995705f1299",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure the datetime column is in the correct format (if not already)\n",
    "metro['real_arrival'] = pd.to_datetime(metro['real_arrival'])\n",
    "\n",
    "# Extract hour from the datetime column\n",
    "metro['hour'] = metro['real_arrival'].dt.hour\n",
    "\n",
    "# Group by hour and calculate the average pourc_validations\n",
    "hourly_validations = metro.groupby('hour')['pourc_validations'].mean().reset_index()\n",
    "\n",
    "# Plot the hourly average pourc_validations\n",
    "plt.figure(figsize=(14, 6))\n",
    "sns.lineplot(\n",
    "    data=hourly_validations,\n",
    "    x='hour',\n",
    "    y='pourc_validations',\n",
    "    marker='o',\n",
    "    color='b'  # Blue color for the line\n",
    ")\n",
    "\n",
    "# Add labels and title\n",
    "plt.xlabel('Hour of Day')\n",
    "plt.ylabel('Average Pourc Validations')\n",
    "plt.title('Average Pourc Validations Per Hour of Day')\n",
    "\n",
    "# Set the x-ticks to be from 0 to 23 (representing hours)\n",
    "plt.xticks(range(24))\n",
    "\n",
    "# Add grid for better readability\n",
    "plt.grid(True)\n",
    "\n",
    "# Show the plot\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f03fb2fb-dc05-4283-944f-501f0eaa704e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
