{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3a7b75c8-77e6-4f8a-b6fa-43e26160a076",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import libraries\n",
    "import requests\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import os\n",
    "import html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9dfacd7b-e322-4702-a42d-fc89b2d1e45f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check what to do with validations-reseau.csv?\n",
    "#Need PCA bc units are different for variabless"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f252aae8-65d5-4975-84a6-820929be139b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Download dataset\n",
    "df_lines = pd.read_csv(\"./Online_Data/referentiel-des-lignes.csv\", sep=';')\n",
    "stops_data = pd.read_csv(\"./Online_Data/arrets.csv\", sep=';')\n",
    "\n",
    "trafic2023_ratio = pd.read_csv(\"./Online_Data/validations-1er-semestre.csv\", sep=';')\n",
    "trafic2023_raw = pd.read_csv(\"./Online_Data/validations-reseau.csv\", sep=\";\")\n",
    "\n",
    "hourly_weather = pd.read_csv(\"./Online_Data/Weather/hourly_weather.csv\")\n",
    "minutely_15_weather = pd.read_csv(\"./Online_Data/Weather/minutely_15_weather.csv\")\n",
    "\n",
    "metro_incident = pd.read_csv(\"./Collected_Data/metro_line_reports.csv\")\n",
    "rer_incident = pd.read_csv(\"./Collected_Data/rer_line_reports.csv\")\n",
    "\n",
    "df_holidays = pd.read_csv(\"./Online_Data/vacances-scolaires-par-zone.csv\", sep = \";\")\n",
    "df_bank_holidays = pd.read_csv(\"./Online_Data/jours_feries_metropole.csv\", sep=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fe370d35-d4db-450b-b0cf-b94ed319956e",
   "metadata": {},
   "outputs": [],
   "source": [
    "delays_metro = pd.read_csv(\"./Collected_Data/metro_delays.csv\")\n",
    "delays_rer = pd.read_csv(\"./Collected_Data/rail_delays.csv\")\n",
    "onTime_metro = pd.read_csv(\"./Collected_Data/metro_onTime.csv\", low_memory=False)\n",
    "onTime_rer = pd.read_csv(\"./Collected_Data/rail_onTime.csv\", low_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e476fed8-9a20-4835-88ef-b45843de089f",
   "metadata": {},
   "outputs": [],
   "source": [
    "delays_metro2 = pd.read_csv(\"./Collected_Data/metro_delays2.csv\")\n",
    "delays_rer2 = pd.read_csv(\"./Collected_Data/rail_delays2.csv\")\n",
    "onTime_metro2 = pd.read_csv(\"./Collected_Data/metro_onTime2.csv\", low_memory=False)\n",
    "onTime_rer2 = pd.read_csv(\"./Collected_Data/rail_onTime2.csv\", low_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a78675fc-bb71-4e51-b4f3-bd55c6fe5e2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "delays_metro = pd.concat([delays_metro, delays_metro2], ignore_index=True)\n",
    "delays_rer = pd.concat([delays_rer, delays_rer2], ignore_index=True)\n",
    "onTime_metro = pd.concat([onTime_metro, onTime_metro2], ignore_index=True)\n",
    "onTime_rer = pd.concat([onTime_rer, onTime_rer2], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e1d30d5c-4b20-4ae6-b52b-4ed202aa4cf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create filter\n",
    "stops_filter = [22086, 463013, 22136, 462993, 21964, 462969, 22125, 463113, 41295, 473921, 473993, 41354, 474060, 474061]\n",
    "name_filter = [\"CH.D.G.ETOILE\", \"CHATELET\", \"SAINT-LAZARE\",\"ST-GERM.D.PRES\", \"BLANCHE\", \"AVENUE DU PRESIDENT KENNEDY\", \"BUNO GIRONVILLE\", \"MASSY PALAISEAU\"]\n",
    "lines_filter = [\"C01371\", \"C01372\", \"C01374\" ,\" C01382\", \"C01742\", \"C01743\", \"C01727\", \"C0172\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b5fc7e94",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Clean datasets\n",
    "line_refs = df_lines[(~df_lines['TransportSubmode'].isin(['suburbanRailway', 'regionalRail', 'railShuttle']))]\n",
    "line_refs = line_refs[['ID_Line', 'TransportMode', 'Name_Line']]\n",
    "line_refs = line_refs.sort_values(by='ID_Line')\n",
    "\n",
    "stops_data = stops_data[stops_data['ArRType'].isin(['metro', 'rail'])]\n",
    "stops_data = stops_data.sort_values(by=['ArRType', 'ArRId'])\n",
    "stops_data = stops_data[['ArRId', 'ArRName', 'ArRType', 'ArRTown']]\n",
    "\n",
    "def cleaning_message(df):\n",
    "    df['ref'] = df['ref'].str.replace('stop_point:IDFM:', '', regex=False)\n",
    "    df['ref'] = df['ref'].str.replace('line:IDFM:', '', regex=False)\n",
    "    df['ref'] = df['ref'].str.replace('stop_area:IDFM:', '', regex=False)\n",
    "    df['message_text'] = df['message_text'].str.replace('<p>', '', regex=False)\n",
    "    df['message_text'] = df['message_text'].str.replace('<br>', '', regex=False) \n",
    "    df = df[df[\"channel_name\"].isin([\"moteur\"])]\n",
    "\n",
    "cleaning_message(metro_incident)\n",
    "cleaning_message(rer_incident)\n",
    "\n",
    "metro_incident['message_text'] = metro_incident['message_text'].str.split('</p>').str[0]\n",
    "metro_incident.drop_duplicates(subset=[\"disruption_id\"], inplace=True)\n",
    "metro_incident.drop(['tags', 'category', 'updated_at', 'channel_name'], axis=1, inplace=True) \n",
    "\n",
    "rer_incident['message_text'] = rer_incident['message_text'].str.replace('</p>', '', regex=False)\n",
    "rer_incident['message_text'] = rer_incident['message_text'].apply(html.unescape)\n",
    "rer_incident.drop_duplicates(subset=[\"disruption_id\"], inplace=True)\n",
    "rer_incident.drop(['tags', 'category', 'updated_at', 'channel_name'], axis=1, inplace=True) \n",
    "\n",
    "def refs(df):\n",
    "    df['line_ref'] = df['line_ref'].str.replace('STIF:Line::', '', regex=False).str.rstrip(':')\n",
    "    df['stop_reference'] = pd.to_numeric(df['stop_reference'], errors='coerce')\n",
    "    df['stop_reference'] = df['stop_reference'].fillna(0).astype('int64')\n",
    "\n",
    "refs(delays_metro)\n",
    "refs(delays_rer)\n",
    "refs(onTime_metro)\n",
    "refs(onTime_rer)\n",
    "\n",
    "delays_metro.drop(['scheduled_arrival','scheduled_departure','arrival_difference','departure_difference'], axis=1, inplace=True)\n",
    "onTime_metro.drop(['scheduled_arrival','scheduled_departure','arrival_difference','departure_difference'], axis=1, inplace=True)\n",
    "\n",
    "mapping = dict(zip(name_filter, stops_filter))\n",
    "trafic2023_ratio['LIBELLE_ARRET_REA'] = trafic2023_ratio['LIBELLE_ARRET'].replace(mapping) #issue -  incorrect mapping? stop number refers to wrong stop\n",
    "trafic2023_ratio = trafic2023_ratio[trafic2023_ratio['LIBELLE_ARRET_REA'].isin(stops_filter)]\n",
    "\n",
    "#trafic2023_raw['LIBELLE_ARRET_REA'] = trafic2023_raw['LIBELLE_ARRET'].replace(mapping) #don't need anymore?\n",
    "#trafic2023_raw.drop([\"lda\"],axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ba355b65",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Holidays and day types\n",
    "df_bank_holidays[\"Date\"] = pd.to_datetime(df_bank_holidays[\"date\"])\n",
    "df_holidays[\"Date\"] = pd.to_datetime(df_holidays[\"Date\"])\n",
    "\n",
    "df_with_holidays = df_holidays.merge(df_bank_holidays, on = \"Date\", how = \"left\")\n",
    "df_with_holidays = df_with_holidays.sort_values(by='Date')\n",
    "\n",
    "df_with_holidays['holiday_type'] = df_with_holidays['nom_jour_ferie'].combine_first(df_with_holidays['Nom de la période'])\n",
    "df_with_holidays['day_of_week'] = df_with_holidays['Date'].dt.weekday\n",
    "\n",
    "def classify_day(row):\n",
    "    if row['day_of_week'] < 5:  # Weekdays (Monday to Friday)\n",
    "        if pd.notna(row['holiday_type']):\n",
    "            return 'JOVS'  # Weekday with a holiday\n",
    "        else:\n",
    "            return 'JOHV'  # Weekday without a holiday\n",
    "    elif row['day_of_week'] == 5:  # Saturday\n",
    "        if pd.notna(row['holiday_type']):\n",
    "            return 'SAVS'  # Saturday with a holiday\n",
    "        else:\n",
    "            return 'SAHV'  # Saturday without a holiday\n",
    "    elif row['day_of_week'] == 6:  # Sunday\n",
    "        return 'DIJFP'  # Sunday (always labeled DIJFP)\n",
    "    return None\n",
    "\n",
    "df_with_holidays['day_type'] = df_with_holidays.apply(classify_day, axis=1)\n",
    "df_with_holidays[\"is_bank_holiday\"] = (df_with_holidays[\"nom_jour_ferie\"]).notna().astype(int)\n",
    "df_with_holidays[\"is_holiday\"] = (df_with_holidays[\"Nom de la période\"]).notna().astype(int)\n",
    "df_with_holidays[\"saturday\"]= (df_with_holidays[\"Date\"].dt.weekday == 5).astype(int)\n",
    "df_with_holidays[\"sunday\"]=  (df_with_holidays[\"Date\"].dt.weekday == 6).astype(int)\n",
    "df_with_holidays[\"is_weekend\"]= df_with_holidays[\"Date\"].dt.weekday.isin([5,6]).astype(int)\n",
    "df_with_holidays[\"is_weekend_or_bank_holiday\"] = df_with_holidays[[\"is_weekend\", \"is_bank_holiday\"]].max(axis=1)\n",
    "\n",
    "df_with_holidays.drop(['timestamp_unix', 'date', 'annee', 'zone', 'Calendrier Zone A', 'Calendrier Zone B', 'Calendrier Zone C'], axis=1, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "69302686",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Filter dataset\n",
    "stops_data = stops_data[stops_data['ArRId'].isin(stops_filter)]\n",
    "\n",
    "metro_incident = metro_incident[\n",
    "    metro_incident['ref'].isin(map(str, stops_filter + lines_filter))\n",
    "]\n",
    "\n",
    "rer_incident = rer_incident[\n",
    "    rer_incident['ref'].isin(map(str, stops_filter + lines_filter))\n",
    "]\n",
    "\n",
    "trafic2023_ratio[['start_hour', 'end_hour']] = trafic2023_ratio['TRNC_HORR_60'].str.extract(r'(\\d+)H-(\\d+)H').dropna().astype(int)\n",
    "trafic2023_ratio.drop(['CODE_STIF_RES', 'CODE_STIF_ARRET', 'LIBELLE_ARRET_REA', 'lda'], axis=1, inplace=True)\n",
    "\n",
    "trafic2023_ratio_rer = trafic2023_ratio[trafic2023_ratio['CODE_STIF_TRNS'].isin([810, 800])]\n",
    "trafic2023_ratio_metro = trafic2023_ratio[trafic2023_ratio['CODE_STIF_TRNS'].isin([100])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "aeb54104",
   "metadata": {},
   "outputs": [],
   "source": [
    "#don't need?\n",
    "#trafic2023_raw = trafic2023_raw[trafic2023_raw['LIBELLE_ARRET_REA'].isin(stops_filter)] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "afadb29f-88b3-4e5e-a454-3d0d6cc7fd77",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Merge datasets\n",
    "merged_metro = pd.concat([delays_metro, onTime_metro], ignore_index=True)\n",
    "merged_metro = pd.merge(merged_metro, stops_data, left_on='stop_reference', right_on='ArRId')\n",
    "merged_metro.drop(['ArRId', 'ArRName', 'ArRType', 'transport_mode', 'recorded_at_time'], axis=1, inplace=True) \n",
    "\n",
    "merged_rer = pd.concat([delays_rer, onTime_rer], ignore_index=True)\n",
    "merged_rer = pd.merge(merged_rer, stops_data, left_on='stop_reference', right_on='ArRId')\n",
    "merged_rer.drop(['ArRId', 'ArRName', 'ArRType', 'transport_mode', 'recorded_at_time'],  axis=1, inplace=True)\n",
    "\n",
    "def date_format(df):\n",
    "    df['timestamp'] = pd.to_datetime(df['timestamp'].str.replace('Z', ''), errors='coerce')\n",
    "    df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
    "    df['real_arrival'] = pd.to_datetime(df['real_arrival']).dt.strftime('%Y-%m-%d %H:%M:%S')\n",
    "    df['real_arrival'] = pd.to_datetime(df['real_arrival'])\n",
    "    df['real_departure'] = pd.to_datetime(df['real_departure']).dt.strftime('%Y-%m-%d %H:%M:%S')\n",
    "    df['real_departure'] = pd.to_datetime(df['real_departure'])\n",
    "\n",
    "\n",
    "def format_rer(df):\n",
    "    df['scheduled_arrival'] = pd.to_datetime(df['scheduled_arrival']).dt.strftime('%Y-%m-%d %H:%M:%S')\n",
    "    df['scheduled_arrival'] = pd.to_datetime(df['scheduled_arrival'])\n",
    "    df['scheduled_departure'] = pd.to_datetime(df['scheduled_departure']).dt.strftime('%Y-%m-%d %H:%M:%S')\n",
    "    df['scheduled_departure'] = pd.to_datetime(df['scheduled_departure'])\n",
    "    df.loc[(df['arrival_difference'] >= 3) | (df['departure_difference'] >= 3), 'departure_status'] = 'delayed'\n",
    "    df['nearest_datetime'] = df['scheduled_arrival'].combine_first(df['scheduled_departure'])\n",
    "\n",
    "date_format(merged_metro)\n",
    "date_format(merged_rer)\n",
    "format_rer(merged_rer)\n",
    "\n",
    "merged_metro = merged_metro.sort_values(by='real_arrival')\n",
    "merged_rer = merged_rer.sort_values(by='nearest_datetime')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2cd4d3e7-3e72-4853-9080-8995cf45bea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Clean Gas Price data\n",
    "def clean_gas_data(file_path, fuel_type):\n",
    "    df = pd.read_csv(file_path, skiprows=3, sep=';')\n",
    "    df.rename(columns={df.columns[1]: f'{fuel_type}'}, inplace=True)\n",
    "    df = df.iloc[:, :-1]\n",
    "    return df\n",
    "    \n",
    "gas_95_df = clean_gas_data(\"./Online_Data/Gas/octane_95.csv\", \"95\")\n",
    "gas_98_df = clean_gas_data(\"./Online_Data/Gas/octane_98.csv\", \"98\")\n",
    "gas_e10_df = clean_gas_data(\"./Online_Data/Gas/95-E10.csv\", \"E10\")\n",
    "gazole_df = clean_gas_data(\"./Online_Data/Gas/gazole.csv\", \"gazole\")\n",
    "\n",
    "merged_rer['Période'] = pd.to_datetime(merged_rer['nearest_datetime']).dt.strftime('%Y-%m')\n",
    "merged_rer = merged_rer.merge(gas_95_df, on='Période', how='left')\n",
    "merged_rer = merged_rer.merge(gas_98_df, on='Période', how='left')\n",
    "merged_rer = merged_rer.merge(gas_e10_df, on='Période', how='left')\n",
    "merged_rer = merged_rer.merge(gazole_df, on='Période', how='left')\n",
    "\n",
    "merged_metro.drop(columns=['timestamp'], inplace=True)\n",
    "#merged_rer.drop(columns=['Période', 'timestamp'], inplace=True)\n",
    "merged_rer = merged_rer[~merged_rer.duplicated(subset=merged_rer.columns.difference(['delayed_status']).tolist(), keep='last')]\n",
    "\n",
    "merged_rer.loc[merged_rer['scheduled_arrival'].dt.month == 1, 'E10'] = 1.76\n",
    "merged_rer.loc[merged_rer['scheduled_arrival'].dt.month == 1, '95'] = 1.79\n",
    "merged_rer.loc[merged_rer['scheduled_arrival'].dt.month == 1, '98'] = 1.87\n",
    "merged_rer.loc[merged_rer['scheduled_arrival'].dt.month == 1, 'gazole'] = 1.70"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "577774b0-7de7-4753-a1c0-4c3868fad8dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Weather Variable\n",
    "\n",
    "#hourly_weather['date'] = pd.to_datetime(hourly_weather['date']) - don't need hourly?\n",
    "minutely_15_weather['date'] = pd.to_datetime(minutely_15_weather['date'])\n",
    "\n",
    "merged_metro = pd.merge_asof(merged_metro, \n",
    "                   minutely_15_weather, \n",
    "                   left_on='real_arrival', \n",
    "                   right_on='date', \n",
    "                   direction='nearest')\n",
    "\n",
    "merged_rer = pd.merge_asof(merged_rer, \n",
    "                   minutely_15_weather, \n",
    "                   left_on='nearest_datetime', \n",
    "                   right_on='date', \n",
    "                   direction='nearest')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6b510369",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Add holidays\n",
    "merged_rer['day'] = pd.to_datetime(merged_rer['nearest_datetime']).dt.normalize()\n",
    "merged_rer = merged_rer.merge(df_with_holidays, left_on='day', right_on='Date', how='left')\n",
    "merged_rer['hour'] = merged_rer['nearest_datetime'].dt.hour.astype('int64')\n",
    "\n",
    "merged_metro['day'] = pd.to_datetime(merged_metro['real_arrival']).dt.normalize()\n",
    "merged_metro = merged_metro.merge(df_with_holidays, left_on='day', right_on='Date', how='left')\n",
    "merged_metro['hour'] = merged_metro['real_arrival'].dt.hour.astype('int64')\n",
    "\n",
    "merged_metro = merged_metro.sort_values(by='real_arrival')\n",
    "merged_rer = merged_rer.sort_values(by='nearest_datetime')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "effacf6a-b7b8-44b7-9c61-45b2121dc758",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split by stop\n",
    "CDG = merged_metro[merged_metro['stop_name'].isin(['Charles de Gaulle-Etoile'])]\n",
    "SGP = merged_metro[merged_metro['stop_name'].isin(['Saint-Germain des Prés'])]\n",
    "BL = merged_metro[merged_metro['stop_name'].isin(['Blanche'])]\n",
    "SL = merged_metro[merged_metro['stop_name'].isin(['Saint-Lazare'])]\n",
    "APK = merged_rer[merged_rer['stop_name'].isin(['Avenue du Président Kennedy'])]\n",
    "CLH = merged_rer[merged_rer['stop_name'].isin(['Châtelet - Les Halles'])]\n",
    "GBG = merged_rer[merged_rer['stop_name'].isin(['Gare de Buno Gironville'])]\n",
    "MP = merged_rer[merged_rer['stop_name'].isin(['Massy - Palaiseau'])]\n",
    "\n",
    "CDG = CDG.drop_duplicates(subset=['destination_name', 'real_arrival'], keep='first')\n",
    "SGP = SGP.drop_duplicates(subset=['destination_name', 'real_arrival'], keep='first')\n",
    "BL = BL.drop_duplicates(subset=['destination_name', 'real_arrival'], keep='first')\n",
    "SL = SL.drop_duplicates(subset=['destination_name', 'real_arrival'], keep='first')\n",
    "\n",
    "APK = APK.drop_duplicates(subset=['destination_name', 'scheduled_arrival'], keep='first')\n",
    "CLH = CLH.drop_duplicates(subset=['destination_name', 'scheduled_arrival'], keep='first')\n",
    "GBG = GBG.drop_duplicates(subset=['destination_name', 'scheduled_arrival'], keep='first')\n",
    "MP = MP.drop_duplicates(subset=['destination_name', 'scheduled_arrival'], keep='first')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6574822c",
   "metadata": {},
   "outputs": [],
   "source": [
    "CDG_traffic = trafic2023_ratio_metro[trafic2023_ratio_metro['LIBELLE_ARRET'] == 'CH.D.G.ETOILE']\n",
    "SGP_traffic = trafic2023_ratio_metro[trafic2023_ratio_metro['LIBELLE_ARRET'] == 'ST-GERM.D.PRES']\n",
    "BL_traffic = trafic2023_ratio_metro[trafic2023_ratio_metro['LIBELLE_ARRET'] == 'BLANCHE']\n",
    "SL_traffic = trafic2023_ratio_metro[trafic2023_ratio_metro['LIBELLE_ARRET'] == 'SAINT-LAZARE']\n",
    "APK_traffic = trafic2023_ratio_rer[trafic2023_ratio_rer['LIBELLE_ARRET'] == 'AVENUE DU PRESIDENT KENNEDY']\n",
    "CLH_traffic = trafic2023_ratio_rer[trafic2023_ratio_rer['LIBELLE_ARRET'] == 'CHATELET']\n",
    "GBG_traffic = trafic2023_ratio_rer[trafic2023_ratio_rer['LIBELLE_ARRET'] == 'BUNO GIRONVILLE']\n",
    "MP_traffic = trafic2023_ratio_rer[trafic2023_ratio_rer['LIBELLE_ARRET'] == 'MASSY PALAISEAU']\n",
    "\n",
    "CDG = CDG.merge(CDG_traffic, left_on=['day_type', 'hour'], right_on=['CAT_JOUR', 'start_hour'], how='left')\n",
    "SGP = SGP.merge(SGP_traffic, left_on=['day_type', 'hour'], right_on=['CAT_JOUR', 'start_hour'], how='left')\n",
    "BL = BL.merge(BL_traffic, left_on=['day_type', 'hour'], right_on=['CAT_JOUR', 'start_hour'], how='left')\n",
    "SL = SL.merge(SL_traffic, left_on=['day_type', 'hour'], right_on=['CAT_JOUR', 'start_hour'], how='left')\n",
    "APK = APK.merge(APK_traffic, left_on=['day_type', 'hour'], right_on=['CAT_JOUR', 'start_hour'], how='left')\n",
    "CLH = CLH.merge(CLH_traffic, left_on=['day_type', 'hour'], right_on=['CAT_JOUR', 'start_hour'], how='left')\n",
    "GBG = GBG.merge(GBG_traffic, left_on=['day_type', 'hour'], right_on=['CAT_JOUR', 'start_hour'], how='left')\n",
    "MP = MP.merge(MP_traffic, left_on=['day_type', 'hour'], right_on=['CAT_JOUR', 'start_hour'], how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0ad358c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Drop uneccesary columns\n",
    "dfs = [CDG, SGP, BL, SL, APK, CLH, GBG, MP]\n",
    "\n",
    "for df in dfs:\n",
    "    df.drop(columns=['stop_reference', 'line_ref', 'day', 'date', 'Nom de la période', 'nom_jour_ferie', 'holiday_type', 'hour', 'CODE_STIF_TRNS', 'LIBELLE_ARRET', 'CAT_JOUR', 'TRNC_HORR_60', 'start_hour', 'end_hour'], inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9dc91dfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#To csv\n",
    "for df, name in zip(dfs, ['CDG', 'SGP', 'BL', 'SL', 'APK', 'CLH', 'GBG', 'MP']):\n",
    "    df.to_csv(f'./Stations/{name}.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b1fa3797",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(42906, 35)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CLH.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "18b59b7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#rer_incident['timestamp'] = pd.to_datetime(rer_incident['timestamp'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0c384ed",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# travaux dummy?"
   ]
  },
  {
   "cell_type": "raw",
   "id": "0a362c02-b029-4ab7-837f-7adedc7a98ea",
   "metadata": {},
   "source": [
    "import re\n",
    "from dateutil.parser import parse\n",
    "\n",
    "df = pd.merge_asof(merged_rer, \n",
    "                   rer_incident, \n",
    "                   left_on='timestamp', \n",
    "                   right_on='timestamp', \n",
    "                   direction='nearest')\n",
    "\n",
    "# Preprocess French weekdays and months\n",
    "french_weekdays = {\n",
    "    \"lundi\": \"Monday\", \"mardi\": \"Tuesday\", \"mercredi\": \"Wednesday\", \"jeudi\": \"Thursday\",\n",
    "    \"vendredi\": \"Friday\", \"samedi\": \"Saturday\", \"dimanche\": \"Sunday\"\n",
    "}\n",
    "\n",
    "# Function to preprocess dates\n",
    "def preprocess_date(date_str):\n",
    "    date_str = re.sub(r'\\b1er\\b', '1', date_str, flags=re.IGNORECASE)\n",
    "    for french_day, english_day in french_weekdays.items():\n",
    "        date_str = date_str.replace(french_day, english_day)\n",
    "    try:\n",
    "        return parse(date_str, dayfirst=True)\n",
    "    except Exception:\n",
    "        return None  # Return None for unparseable dates\n",
    "\n",
    "# Check if travaux is in effect\n",
    "def check_travaux_in_effect(message_text, current_datetime):\n",
    "    if not isinstance(message_text, str):\n",
    "        return 0\n",
    "\n",
    "    valid = False\n",
    "\n",
    "    # 1. Parse full date ranges\n",
    "    date_range_match = re.findall(r'du ([\\w\\s]+) au ([\\w\\s]+)', message_text, re.IGNORECASE)\n",
    "    for start_date_str, end_date_str in date_range_match:\n",
    "        try:\n",
    "            start_date = preprocess_date(start_date_str)\n",
    "            end_date = preprocess_date(end_date_str)\n",
    "            if start_date and end_date and start_date <= current_datetime.date() <= end_date:\n",
    "                valid = True\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    # 2. Parse specific dates\n",
    "    specific_days_match = re.search(r'(dimanches?|lundis?|mardis?|mercredis?|jeudis?|vendredis?|samedis?) (\\d+(?:, \\d+)*)(?: et le (\\w+ \\d+))?', message_text, re.IGNORECASE)\n",
    "    if specific_days_match:\n",
    "        day_list = list(map(int, specific_days_match.group(2).split(',')))\n",
    "        additional_date = specific_days_match.group(3)\n",
    "        if additional_date:\n",
    "            additional_date = preprocess_date(additional_date)\n",
    "            if additional_date:\n",
    "                day_list.append(additional_date.day)\n",
    "        if current_datetime.day in day_list:\n",
    "            valid = True\n",
    "\n",
    "    # 3. Parse recurring patterns\n",
    "    recurring_match = re.search(r'du (\\w+) au (\\w+)', message_text, re.IGNORECASE)\n",
    "    if recurring_match:\n",
    "        start_day = french_weekdays.get(recurring_match.group(1).lower(), \"\").capitalize()\n",
    "        end_day = french_weekdays.get(recurring_match.group(2).lower(), \"\").capitalize()\n",
    "        weekdays = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']\n",
    "        if start_day in weekdays and end_day in weekdays:\n",
    "            start_index = weekdays.index(start_day)\n",
    "            end_index = weekdays.index(end_day)\n",
    "            if start_index <= weekdays.index(current_datetime.strftime('%A')) <= end_index:\n",
    "                valid = True\n",
    "\n",
    "    # 4. Parse time ranges\n",
    "    time_match = re.search(r'Ã partir de (\\d{1,2}h\\d{2})', message_text, re.IGNORECASE)\n",
    "    if time_match:\n",
    "        start_time_str = time_match.group(1).replace('h', ':')\n",
    "        try:\n",
    "            start_time = datetime.strptime(start_time_str, '%H:%M').time()\n",
    "            if current_datetime.time() >= start_time:\n",
    "                valid = True\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    return 1 if valid else 0\n",
    "\n",
    "# Apply the function\n",
    "df['travaux_in_effect'] = df.apply(\n",
    "    lambda row: check_travaux_in_effect(row['message_text'], row['Date']), axis=1\n",
    ")\n",
    "\n",
    "# Display the result\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "b0d358e2-c15f-443b-b5c7-9dd9e80576cb",
   "metadata": {},
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from datetime import datetime, timedelta\n",
    "from dateutil.parser import parse\n",
    "\n",
    "# Month and day translations from French to English\n",
    "MONTHS = {\n",
    "    \"janvier\": \"January\", \"février\": \"February\", \"mars\": \"March\",\n",
    "    \"avril\": \"April\", \"mai\": \"May\", \"juin\": \"June\",\n",
    "    \"juillet\": \"July\", \"août\": \"August\", \"septembre\": \"September\",\n",
    "    \"octobre\": \"October\", \"novembre\": \"November\", \"décembre\": \"December\"\n",
    "}\n",
    "\n",
    "DAYS = {\n",
    "    \"lundi\": \"Monday\", \"mardi\": \"Tuesday\", \"mercredi\": \"Wednesday\",\n",
    "    \"jeudi\": \"Thursday\", \"vendredi\": \"Friday\", \"samedi\": \"Saturday\", \"dimanche\": \"Sunday\"\n",
    "}\n",
    "\n",
    "def preprocess_date(date_str, reference_year=None):\n",
    "    try:\n",
    "        # Replace French month/day names with English equivalents\n",
    "        for fr, en in {**MONTHS, **DAYS}.items():\n",
    "            date_str = date_str.replace(fr, en)\n",
    "        \n",
    "        # Parse the date\n",
    "        date = parse(date_str, dayfirst=True, fuzzy=True)\n",
    "        return date\n",
    "    except Exception:\n",
    "        # Handle cases with no year (default to reference year)\n",
    "        if reference_year:\n",
    "            for month in MONTHS.keys():\n",
    "                if month in date_str:\n",
    "                    date_str += f\" {reference_year}\"\n",
    "                    return parse(date_str, dayfirst=True, fuzzy=True)\n",
    "        \n",
    "        # Handle standalone weekdays (return nearest future date)\n",
    "        today = datetime.today()\n",
    "        if date_str.strip() in DAYS.keys():\n",
    "            target_weekday = DAYS[date_str.strip()]\n",
    "            current_weekday = today.strftime('%A')\n",
    "            days_ahead = (list(DAYS.values()).index(target_weekday) - \n",
    "                          list(DAYS.values()).index(current_weekday)) % 7\n",
    "            return today + timedelta(days=days_ahead)\n",
    "\n",
    "        # Raise an error for unresolved formats\n",
    "        raise ValueError(f\"Cannot parse date string: {date_str}\")\n",
    "\n",
    "# Function to extract affected date ranges\n",
    "def extract_affected_dates(message):\n",
    "    start_datetime = None\n",
    "    end_datetime = None\n",
    "\n",
    "    # Match \"du ... au ...\" with optional times\n",
    "    match = re.search(r'du ([\\w\\s]+) au ([\\w\\s]+)(?:, de (\\d{1,2}h\\d{2}) à (\\d{1,2}h\\d{2}))?', message, re.IGNORECASE)\n",
    "    if match:\n",
    "        try:\n",
    "            start_date_str, end_date_str, start_time_str, end_time_str = match.groups()\n",
    "            start_date = preprocess_date(start_date_str)\n",
    "            end_date = preprocess_date(end_date_str)\n",
    "            \n",
    "            if start_time_str:\n",
    "                start_time = datetime.strptime(start_time_str.replace('h', ':'), '%H:%M').time()\n",
    "                start_datetime = datetime.combine(start_date, start_time)\n",
    "            else:\n",
    "                start_datetime = datetime.combine(start_date, datetime.min.time())\n",
    "            \n",
    "            if end_time_str:\n",
    "                end_time = datetime.strptime(end_time_str.replace('h', ':'), '%H:%M').time()\n",
    "                end_datetime = datetime.combine(end_date, end_time)\n",
    "            else:\n",
    "                end_datetime = datetime.combine(end_date, datetime.max.time())\n",
    "        except Exception as e:\n",
    "            print(f\"Error parsing date range: {e}\")\n",
    "    return start_datetime, end_datetime\n",
    "\n",
    "# Apply extraction to rer_incidents\n",
    "rer_incident[['start_datetime', 'end_datetime']] = rer_incident['message_text'].apply(\n",
    "    lambda x: pd.Series(extract_affected_dates(x))\n",
    ")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "cdaeb994-1fbc-46ce-88ec-1fee79028453",
   "metadata": {},
   "source": [
    "\n",
    "# Merge rer_incidents into merged_rer based on nearest date\n",
    "merged_rer = pd.merge_asof(\n",
    "    merged_rer.sort_values('date'),\n",
    "    rer_incidents[['start_datetime', 'end_datetime']],\n",
    "    left_on='date', \n",
    "    right_on='start_datetime',\n",
    "    direction='backward'\n",
    ")\n",
    "\n",
    "# Create dummy variable for applicable dates\n",
    "merged_rer['travaux_in_effect'] = merged_rer.apply(\n",
    "    lambda row: 1 if row['start_datetime'] <= row['date'] <= row['end_datetime'] else 0, axis=1\n",
    ")\n",
    "\n",
    "# Display the result\n",
    "print(merged_rer)\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "41551158-07c6-476f-a0aa-c79dccf10ec6",
   "metadata": {},
   "source": [
    "def extract_datetime(message_text):\n",
    "    start_datetime = None\n",
    "    end_datetime = None\n",
    "    datetime_ranges = []\n",
    "\n",
    "    try:\n",
    "        # Replace \"inclus\" with nothing, for simplicity in parsing\n",
    "        message_text = message_text.replace(\"inclus\", \"\").strip()\n",
    "\n",
    "        # Handle full date ranges: \"du 06/01 au 21/03/2025\"\n",
    "        date_range_match = re.search(r'du (\\d{1,2}/\\d{1,2}/\\d{4}) au (\\d{1,2}/\\d{1,2}/\\d{4})', message_text, re.IGNORECASE)\n",
    "        if date_range_match:\n",
    "            start_date_str = date_range_match.group(1)\n",
    "            end_date_str = date_range_match.group(2)\n",
    "            start_datetime = parse(start_date_str, dayfirst=True)\n",
    "            end_datetime = parse(end_date_str, dayfirst=True)\n",
    "            datetime_ranges.append((start_datetime, end_datetime))\n",
    "\n",
    "        # Handle exclusions within ranges: \"sauf les 31 January / 14,17, 21 et 24 February\"\n",
    "        exceptions_match = re.search(r'sauf les (.+)', message_text, re.IGNORECASE)\n",
    "        if exceptions_match:\n",
    "            exceptions = exceptions_match.group(1)\n",
    "            # Process exceptions if necessary (can create exclusions)\n",
    "\n",
    "        # Handle specific dates: \"le 31 January\"\n",
    "        specific_dates_match = re.findall(r'le (\\d{1,2} \\w+ \\d{4})', message_text, re.IGNORECASE)\n",
    "        for specific_date in specific_dates_match:\n",
    "            specific_datetime = parse(translate_french_date(specific_date), dayfirst=True)\n",
    "            datetime_ranges.append((specific_datetime, specific_datetime + timedelta(hours=23, minutes=59)))\n",
    "\n",
    "        # Handle recurring dates: \"les Saturdays, Sundays\"\n",
    "        recurring_days_match = re.search(r'les (.+)', message_text, re.IGNORECASE)\n",
    "        if recurring_days_match:\n",
    "            recurring_days = recurring_days_match.group(1).split(',')\n",
    "            # Process recurring days if needed\n",
    "\n",
    "        # Handle partial times: \"à partir de 22h45\"\n",
    "        time_range_match = re.search(r'à partir de (\\d{1,2}h\\d{2})', message_text, re.IGNORECASE)\n",
    "        if time_range_match:\n",
    "            start_time = time_range_match.group(1).replace('h', ':')\n",
    "            end_time = \"23:59\"  # Default end time for \"à partir de\"\n",
    "            # Merge this time with detected dates for datetime ranges\n",
    "\n",
    "        # Handle default \"toute la journée\"\n",
    "        if \"toute la journée\" in message_text:\n",
    "            # Assume full-day ranges for detected dates\n",
    "\n",
    "        # Set the earliest start and latest end times if ranges exist\n",
    "            if datetime_ranges:\n",
    "                start_datetime = min([start for start, end in datetime_ranges])\n",
    "                end_datetime = max([end for start, end in datetime_ranges])\n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting datetime: {e}\\nMessage: {message_text}\")\n",
    "\n",
    "    return start_datetime, end_datetime\n",
    "\n",
    "\n",
    "\n",
    "# Apply the updated function to the DataFrame\n",
    "rer_incident[['start_datetime', 'end_datetime']] = rer_incident['message_text'].apply(\n",
    "    lambda msg: pd.Series(extract_datetime(msg))\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af2b1795",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
