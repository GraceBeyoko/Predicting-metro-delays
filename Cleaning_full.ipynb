{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3a7b75c8-77e6-4f8a-b6fa-43e26160a076",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import libraries\n",
    "import requests\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import os\n",
    "import html\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f252aae8-65d5-4975-84a6-820929be139b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Download datasets\n",
    "df_lines = pd.read_csv(\"./Online_Data/referentiel-des-lignes.csv\", sep=';')\n",
    "stops_data = pd.read_csv(\"./Online_Data/arrets.csv\", sep=';')\n",
    "\n",
    "trafic2023_ratio = pd.read_csv(\"./Online_Data/validations-1er-semestre.csv\", sep=';')\n",
    "\n",
    "hourly_weather = pd.read_csv(\"./Online_Data/Weather/hourly_weather.csv\")\n",
    "minutely_15_weather = pd.read_csv(\"./Online_Data/Weather/minutely_15_weather.csv\")\n",
    "\n",
    "df_holidays = pd.read_csv(\"./Online_Data/vacances-scolaires-par-zone.csv\", sep = \";\")\n",
    "df_bank_holidays = pd.read_csv(\"./Online_Data/jours_feries_metropole.csv\", sep=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fe370d35-d4db-450b-b0cf-b94ed319956e",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './Collected_Data/metro_delays.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m delays_metro \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./Collected_Data/metro_delays.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      2\u001b[0m delays_rer \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./Collected_Data/rail_delays.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      3\u001b[0m onTime_metro \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./Collected_Data/metro_onTime.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m, low_memory\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/pandas/io/parsers/readers.py:948\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m    935\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m    936\u001b[0m     dialect,\n\u001b[1;32m    937\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    944\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[1;32m    945\u001b[0m )\n\u001b[1;32m    946\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m--> 948\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/pandas/io/parsers/readers.py:611\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    608\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    610\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 611\u001b[0m parser \u001b[38;5;241m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[1;32m    613\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[1;32m    614\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1448\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1445\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   1447\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1448\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_engine(f, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1705\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1703\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[1;32m   1704\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1705\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m get_handle(\n\u001b[1;32m   1706\u001b[0m     f,\n\u001b[1;32m   1707\u001b[0m     mode,\n\u001b[1;32m   1708\u001b[0m     encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[1;32m   1709\u001b[0m     compression\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompression\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[1;32m   1710\u001b[0m     memory_map\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmemory_map\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m),\n\u001b[1;32m   1711\u001b[0m     is_text\u001b[38;5;241m=\u001b[39mis_text,\n\u001b[1;32m   1712\u001b[0m     errors\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding_errors\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstrict\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m   1713\u001b[0m     storage_options\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstorage_options\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[1;32m   1714\u001b[0m )\n\u001b[1;32m   1715\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1716\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/pandas/io/common.py:863\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    858\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    859\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[1;32m    860\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[1;32m    861\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[1;32m    862\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[0;32m--> 863\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(\n\u001b[1;32m    864\u001b[0m             handle,\n\u001b[1;32m    865\u001b[0m             ioargs\u001b[38;5;241m.\u001b[39mmode,\n\u001b[1;32m    866\u001b[0m             encoding\u001b[38;5;241m=\u001b[39mioargs\u001b[38;5;241m.\u001b[39mencoding,\n\u001b[1;32m    867\u001b[0m             errors\u001b[38;5;241m=\u001b[39merrors,\n\u001b[1;32m    868\u001b[0m             newline\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    869\u001b[0m         )\n\u001b[1;32m    870\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    871\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[1;32m    872\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './Collected_Data/metro_delays.csv'"
     ]
    }
   ],
   "source": [
    "delays_metro = pd.read_csv(\"./Collected_Data/metro_delays.csv\")\n",
    "delays_rer = pd.read_csv(\"./Collected_Data/rail_delays.csv\")\n",
    "onTime_metro = pd.read_csv(\"./Collected_Data/metro_onTime.csv\", low_memory=False)\n",
    "onTime_rer = pd.read_csv(\"./Collected_Data/rail_onTime.csv\", low_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e476fed8-9a20-4835-88ef-b45843de089f",
   "metadata": {},
   "outputs": [],
   "source": [
    "delays_metro2 = pd.read_csv(\"./Collected_Data/metro_delays2.csv\")\n",
    "delays_rer2 = pd.read_csv(\"./Collected_Data/rail_delays2.csv\")\n",
    "onTime_metro2 = pd.read_csv(\"./Collected_Data/metro_onTime2.csv\", low_memory=False)\n",
    "onTime_rer2 = pd.read_csv(\"./Collected_Data/rail_onTime2.csv\", low_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a78675fc-bb71-4e51-b4f3-bd55c6fe5e2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Merge too big split datasets\n",
    "delays_metro = pd.concat([delays_metro, delays_metro2], ignore_index=True)\n",
    "delays_rer = pd.concat([delays_rer, delays_rer2], ignore_index=True)\n",
    "onTime_metro = pd.concat([onTime_metro, onTime_metro2], ignore_index=True)\n",
    "onTime_rer = pd.concat([onTime_rer, onTime_rer2], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1d30d5c-4b20-4ae6-b52b-4ed202aa4cf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create filter\n",
    "stops_filter = [22086, 463013, 22136, 462993, 21964, 462969, 22125, 463113, 41295, 473921, 473993, 41354, 474060, 474061]\n",
    "name_filter = [\"CH.D.G.ETOILE\", \"CHATELET\", \"SAINT-LAZARE\",\"ST-GERM.D.PRES\", \"BLANCHE\", \"AVENUE DU PRESIDENT KENNEDY\", \"BUNO GIRONVILLE\", \"MASSY PALAISEAU\"]\n",
    "lines_filter = [\"C01371\", \"C01372\", \"C01374\" ,\" C01382\", \"C01742\", \"C01743\", \"C01727\", \"C0172\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5fc7e94",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Clean datasets\n",
    "line_refs = df_lines[(~df_lines['TransportSubmode'].isin(['suburbanRailway', 'regionalRail', 'railShuttle']))]\n",
    "line_refs = line_refs[['ID_Line', 'TransportMode', 'Name_Line']]\n",
    "line_refs = line_refs.sort_values(by='ID_Line')\n",
    "\n",
    "stops_data = stops_data[stops_data['ArRType'].isin(['metro', 'rail'])]\n",
    "stops_data = stops_data.sort_values(by=['ArRType', 'ArRId'])\n",
    "stops_data = stops_data[['ArRId', 'ArRName', 'ArRType', 'ArRTown']]\n",
    "\n",
    "def refs(df):\n",
    "    df['line_ref'] = df['line_ref'].str.replace('STIF:Line::', '', regex=False).str.rstrip(':')\n",
    "    df['stop_reference'] = pd.to_numeric(df['stop_reference'], errors='coerce')\n",
    "    df['stop_reference'] = df['stop_reference'].fillna(0).astype('int64')\n",
    "\n",
    "refs(delays_metro)\n",
    "refs(delays_rer)\n",
    "refs(onTime_metro)\n",
    "refs(onTime_rer)\n",
    "\n",
    "delays_metro.drop(['scheduled_arrival','scheduled_departure','arrival_difference','departure_difference'], axis=1, inplace=True)\n",
    "onTime_metro.drop(['scheduled_arrival','scheduled_departure','arrival_difference','departure_difference'], axis=1, inplace=True)\n",
    "\n",
    "mapping = dict(zip(name_filter, stops_filter))\n",
    "trafic2023_ratio['LIBELLE_ARRET_REA'] = trafic2023_ratio['LIBELLE_ARRET'].replace(mapping) #issue -  incorrect mapping? stop number refers to wrong stop\n",
    "trafic2023_ratio = trafic2023_ratio[trafic2023_ratio['LIBELLE_ARRET_REA'].isin(stops_filter)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba355b65",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Holidays and day types\n",
    "df_bank_holidays[\"Date\"] = pd.to_datetime(df_bank_holidays[\"date\"])\n",
    "df_holidays[\"Date\"] = pd.to_datetime(df_holidays[\"Date\"])\n",
    "\n",
    "df_with_holidays = df_holidays.merge(df_bank_holidays, on = \"Date\", how = \"left\")\n",
    "df_with_holidays = df_with_holidays.sort_values(by='Date')\n",
    "\n",
    "df_with_holidays['holiday_type'] = df_with_holidays['nom_jour_ferie'].combine_first(df_with_holidays['Nom de la période'])\n",
    "df_with_holidays['day_of_week'] = df_with_holidays['Date'].dt.weekday\n",
    "\n",
    "def classify_day(row):\n",
    "    if row['day_of_week'] < 5:  # Weekdays (Monday to Friday)\n",
    "        if pd.notna(row['holiday_type']):\n",
    "            return 'JOVS'  # Weekday with a holiday\n",
    "        else:\n",
    "            return 'JOHV'  # Weekday without a holiday\n",
    "    elif row['day_of_week'] == 5:  # Saturday\n",
    "        if pd.notna(row['holiday_type']):\n",
    "            return 'SAVS'  # Saturday with a holiday\n",
    "        else:\n",
    "            return 'SAHV'  # Saturday without a holiday\n",
    "    elif row['day_of_week'] == 6:  # Sunday\n",
    "        return 'DIJFP'  # Sunday (always labeled DIJFP)\n",
    "    return None\n",
    "\n",
    "df_with_holidays['day_type'] = df_with_holidays.apply(classify_day, axis=1)\n",
    "df_with_holidays[\"is_bank_holiday\"] = (df_with_holidays[\"nom_jour_ferie\"]).notna().astype(int)\n",
    "df_with_holidays[\"is_holiday\"] = (df_with_holidays[\"Nom de la période\"]).notna().astype(int)\n",
    "df_with_holidays[\"saturday\"]= (df_with_holidays[\"Date\"].dt.weekday == 5).astype(int)\n",
    "df_with_holidays[\"sunday\"]=  (df_with_holidays[\"Date\"].dt.weekday == 6).astype(int)\n",
    "df_with_holidays[\"is_weekend\"]= df_with_holidays[\"Date\"].dt.weekday.isin([5,6]).astype(int)\n",
    "df_with_holidays[\"is_weekend_or_bank_holiday\"] = df_with_holidays[[\"is_weekend\", \"is_bank_holiday\"]].max(axis=1)\n",
    "\n",
    "df_with_holidays.drop(['timestamp_unix', 'date', 'annee', 'zone', 'Calendrier Zone A', 'Calendrier Zone B', 'Calendrier Zone C'], axis=1, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69302686",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Filter dataset\n",
    "stops_data = stops_data[stops_data['ArRId'].isin(stops_filter)]\n",
    "\n",
    "trafic2023_ratio[['start_hour', 'end_hour']] = trafic2023_ratio['TRNC_HORR_60'].str.extract(r'(\\d+)H-(\\d+)H').dropna().astype(int)\n",
    "trafic2023_ratio.drop(['CODE_STIF_RES', 'CODE_STIF_ARRET', 'LIBELLE_ARRET_REA', 'lda'], axis=1, inplace=True)\n",
    "\n",
    "trafic2023_ratio_rer = trafic2023_ratio[trafic2023_ratio['CODE_STIF_TRNS'].isin([810, 800])]\n",
    "trafic2023_ratio_metro = trafic2023_ratio[trafic2023_ratio['CODE_STIF_TRNS'].isin([100])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afadb29f-88b3-4e5e-a454-3d0d6cc7fd77",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Merge datasets\n",
    "merged_metro = pd.concat([delays_metro, onTime_metro], ignore_index=True)\n",
    "merged_metro = pd.merge(merged_metro, stops_data, left_on='stop_reference', right_on='ArRId')\n",
    "merged_metro.drop(['ArRId', 'ArRName', 'ArRType', 'transport_mode', 'recorded_at_time'], axis=1, inplace=True) \n",
    "\n",
    "merged_rer = pd.concat([delays_rer, onTime_rer], ignore_index=True)\n",
    "merged_rer = pd.merge(merged_rer, stops_data, left_on='stop_reference', right_on='ArRId')\n",
    "merged_rer.drop(['ArRId', 'ArRName', 'ArRType', 'transport_mode', 'recorded_at_time'],  axis=1, inplace=True)\n",
    "\n",
    "def date_format(df):\n",
    "    df['timestamp'] = pd.to_datetime(df['timestamp'].str.replace('Z', ''), errors='coerce')\n",
    "    df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
    "    df['real_arrival'] = pd.to_datetime(df['real_arrival']).dt.strftime('%Y-%m-%d %H:%M:%S')\n",
    "    df['real_arrival'] = pd.to_datetime(df['real_arrival'])\n",
    "    df['real_departure'] = pd.to_datetime(df['real_departure']).dt.strftime('%Y-%m-%d %H:%M:%S')\n",
    "    df['real_departure'] = pd.to_datetime(df['real_departure'])\n",
    "    df['nearest_datetime'] = df['real_arrival'].combine_first(df['real_departure'])\n",
    "    \n",
    "\n",
    "def format_rer(df):\n",
    "    df['scheduled_arrival'] = pd.to_datetime(df['scheduled_arrival']).dt.strftime('%Y-%m-%d %H:%M:%S')\n",
    "    df['scheduled_arrival'] = pd.to_datetime(df['scheduled_arrival'])\n",
    "    df['scheduled_departure'] = pd.to_datetime(df['scheduled_departure']).dt.strftime('%Y-%m-%d %H:%M:%S')\n",
    "    df['scheduled_departure'] = pd.to_datetime(df['scheduled_departure'])\n",
    "    df.loc[(df['arrival_difference'] >= 2) | (df['departure_difference'] >= 2), 'departure_status'] = 'delayed'\n",
    "    df['nearest_datetime'] = df['nearest_datetime'].combine_first(df['scheduled_arrival'])\n",
    "    df['nearest_datetime'] = df['nearest_datetime'].combine_first(df['scheduled_departure'])\n",
    "\n",
    "\n",
    "date_format(merged_metro)\n",
    "date_format(merged_rer)\n",
    "format_rer(merged_rer)\n",
    "\n",
    "merged_metro = merged_metro.sort_values(by='real_arrival')\n",
    "merged_rer = merged_rer.sort_values(by='nearest_datetime')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cd4d3e7-3e72-4853-9080-8995cf45bea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Clean Gas Price data\n",
    "def clean_gas_data(file_path, fuel_type):\n",
    "    df = pd.read_csv(file_path, skiprows=3, sep=';')\n",
    "    df.rename(columns={df.columns[1]: f'{fuel_type}'}, inplace=True)\n",
    "    df = df.iloc[:, :-1]\n",
    "    return df\n",
    "    \n",
    "gas_95_df = clean_gas_data(\"./Online_Data/Gas/octane_95.csv\", \"95\")\n",
    "gas_98_df = clean_gas_data(\"./Online_Data/Gas/octane_98.csv\", \"98\")\n",
    "gas_e10_df = clean_gas_data(\"./Online_Data/Gas/95-E10.csv\", \"E10\")\n",
    "gazole_df = clean_gas_data(\"./Online_Data/Gas/gazole.csv\", \"gazole\")\n",
    "\n",
    "merged_rer['Période'] = pd.to_datetime(merged_rer['nearest_datetime']).dt.strftime('%Y-%m')\n",
    "merged_rer = merged_rer.merge(gas_95_df, on='Période', how='left')\n",
    "merged_rer = merged_rer.merge(gas_98_df, on='Période', how='left')\n",
    "merged_rer = merged_rer.merge(gas_e10_df, on='Période', how='left')\n",
    "merged_rer = merged_rer.merge(gazole_df, on='Période', how='left')\n",
    "\n",
    "merged_metro.drop(columns=['timestamp'], inplace=True)\n",
    "merged_rer = merged_rer[~merged_rer.duplicated(subset=merged_rer.columns.difference(['delayed_status']).tolist(), keep='last')]\n",
    "\n",
    "merged_rer.loc[merged_rer['nearest_datetime'].dt.month == 1, 'E10'] = 1.76\n",
    "merged_rer.loc[merged_rer['nearest_datetime'].dt.month == 1, '95'] = 1.79\n",
    "merged_rer.loc[merged_rer['nearest_datetime'].dt.month == 1, '98'] = 1.87\n",
    "merged_rer.loc[merged_rer['nearest_datetime'].dt.month == 1, 'gazole'] = 1.70"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "577774b0-7de7-4753-a1c0-4c3868fad8dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Weather Variable\n",
    "minutely_15_weather['date'] = pd.to_datetime(minutely_15_weather['date'])\n",
    "\n",
    "merged_metro = pd.merge_asof(merged_metro, \n",
    "                   minutely_15_weather, \n",
    "                   left_on='nearest_datetime', \n",
    "                   right_on='date', \n",
    "                   direction='nearest')\n",
    "\n",
    "merged_rer = pd.merge_asof(merged_rer, \n",
    "                   minutely_15_weather, \n",
    "                   left_on='nearest_datetime', \n",
    "                   right_on='date', \n",
    "                   direction='nearest')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b510369",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Add holidays\n",
    "merged_rer['day'] = pd.to_datetime(merged_rer['nearest_datetime']).dt.normalize()\n",
    "merged_rer = merged_rer.merge(df_with_holidays, left_on='day', right_on='Date', how='left')\n",
    "merged_rer['hour'] = merged_rer['nearest_datetime'].dt.hour.astype('int64')\n",
    "\n",
    "merged_metro['day'] = pd.to_datetime(merged_metro['real_arrival']).dt.normalize()\n",
    "merged_metro = merged_metro.merge(df_with_holidays, left_on='day', right_on='Date', how='left')\n",
    "merged_metro['hour'] = merged_metro['real_arrival'].dt.hour.astype('int64')\n",
    "\n",
    "merged_metro = merged_metro.sort_values(by='real_arrival')\n",
    "merged_rer = merged_rer.sort_values(by='nearest_datetime')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11c6ba17-e814-4019-a867-6ddf650f6f81",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_datetime_features(data, datetime_col):\n",
    "    data[datetime_col] = pd.to_datetime(data[datetime_col])\n",
    "\n",
    "    #Extract datetime components\n",
    "    data[f'{datetime_col}_year'] = data[datetime_col].dt.year\n",
    "    data[f'{datetime_col}_month'] = data[datetime_col].dt.month\n",
    "    data[f'{datetime_col}_day'] = data[datetime_col].dt.day\n",
    "    data[f'{datetime_col}_hour'] = data[datetime_col].dt.hour\n",
    "    data[f'{datetime_col}_minute'] = data[datetime_col].dt.minute\n",
    "    data[f'{datetime_col}_second'] = data[datetime_col].dt.second\n",
    "\n",
    "    #Add cyclical encodings\n",
    "    data[f'{datetime_col}_month_sin'] = np.sin(2 * np.pi * data[f'{datetime_col}_month'] / 12)\n",
    "    data[f'{datetime_col}_month_cos'] = np.cos(2 * np.pi * data[f'{datetime_col}_month'] / 12)\n",
    "    data[f'{datetime_col}_day_sin'] = np.sin(2 * np.pi * data[f'{datetime_col}_day'] / 31)\n",
    "    data[f'{datetime_col}_day_cos'] = np.cos(2 * np.pi * data[f'{datetime_col}_day'] / 31)\n",
    "    data[f'{datetime_col}_hour_sin'] = np.sin(2 * np.pi * data[f'{datetime_col}_hour'] / 24)\n",
    "    data[f'{datetime_col}_hour_cos'] = np.cos(2 * np.pi * data[f'{datetime_col}_hour'] / 24)\n",
    "    data[f'{datetime_col}_minute_sin'] = np.sin(2 * np.pi * data[f'{datetime_col}_minute'] / 60)\n",
    "    data[f'{datetime_col}_minute_cos'] = np.cos(2 * np.pi * data[f'{datetime_col}_minute'] / 60)\n",
    "    data[f'{datetime_col}_second_sin'] = np.sin(2 * np.pi * data[f'{datetime_col}_second'] / 60)\n",
    "    data[f'{datetime_col}_second_cos'] = np.cos(2 * np.pi * data[f'{datetime_col}_second'] / 60)\n",
    "\n",
    "    #Drop intermediate extracted columns\n",
    "    columns_to_drop = [\n",
    "        f'{datetime_col}_year',\n",
    "        f'{datetime_col}_month',\n",
    "        f'{datetime_col}_day',\n",
    "        f'{datetime_col}_hour',\n",
    "        f'{datetime_col}_minute',\n",
    "        f'{datetime_col}_second'\n",
    "    ]\n",
    "    data.drop(columns=columns_to_drop, inplace=True)\n",
    "\n",
    "    return data\n",
    "\n",
    "merged_metro = add_datetime_features(merged_metro, 'nearest_datetime')\n",
    "merged_rer = add_datetime_features(merged_rer, 'nearest_datetime')\n",
    "\n",
    "merged_metro = merged_metro.sort_values(by='nearest_datetime', ascending=True)\n",
    "merged_rer = merged_rer.sort_values(by='nearest_datetime', ascending=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "effacf6a-b7b8-44b7-9c61-45b2121dc758",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split by stop\n",
    "CDG = merged_metro[merged_metro['stop_name'].isin(['Charles de Gaulle-Etoile'])]\n",
    "SGP = merged_metro[merged_metro['stop_name'].isin(['Saint-Germain des Prés'])]\n",
    "BL = merged_metro[merged_metro['stop_name'].isin(['Blanche'])]\n",
    "SL = merged_metro[merged_metro['stop_name'].isin(['Saint-Lazare'])]\n",
    "APK = merged_rer[merged_rer['stop_name'].isin(['Avenue du Président Kennedy'])]\n",
    "CLH = merged_rer[merged_rer['stop_name'].isin(['Châtelet - Les Halles'])]\n",
    "GBG = merged_rer[merged_rer['stop_name'].isin(['Gare de Buno Gironville'])]\n",
    "MP = merged_rer[merged_rer['stop_name'].isin(['Massy - Palaiseau'])]\n",
    "\n",
    "CDG = CDG.drop_duplicates(subset=['destination_name', 'real_arrival'], keep='first')\n",
    "SGP = SGP.drop_duplicates(subset=['destination_name', 'real_arrival'], keep='first')\n",
    "BL = BL.drop_duplicates(subset=['destination_name', 'real_arrival'], keep='first')\n",
    "SL = SL.drop_duplicates(subset=['destination_name', 'real_arrival'], keep='first')\n",
    "\n",
    "APK = APK.drop_duplicates(subset=['destination_name', 'scheduled_arrival'], keep='first')\n",
    "CLH = CLH.drop_duplicates(subset=['destination_name', 'scheduled_arrival'], keep='first')\n",
    "GBG = GBG.drop_duplicates(subset=['destination_name', 'scheduled_arrival'], keep='first')\n",
    "MP = MP.drop_duplicates(subset=['destination_name', 'scheduled_arrival'], keep='first')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6574822c",
   "metadata": {},
   "outputs": [],
   "source": [
    "CDG_traffic = trafic2023_ratio_metro[trafic2023_ratio_metro['LIBELLE_ARRET'] == 'CH.D.G.ETOILE']\n",
    "SGP_traffic = trafic2023_ratio_metro[trafic2023_ratio_metro['LIBELLE_ARRET'] == 'ST-GERM.D.PRES']\n",
    "BL_traffic = trafic2023_ratio_metro[trafic2023_ratio_metro['LIBELLE_ARRET'] == 'BLANCHE']\n",
    "SL_traffic = trafic2023_ratio_metro[trafic2023_ratio_metro['LIBELLE_ARRET'] == 'SAINT-LAZARE']\n",
    "APK_traffic = trafic2023_ratio_rer[trafic2023_ratio_rer['LIBELLE_ARRET'] == 'AVENUE DU PRESIDENT KENNEDY']\n",
    "CLH_traffic = trafic2023_ratio_rer[trafic2023_ratio_rer['LIBELLE_ARRET'] == 'CHATELET']\n",
    "GBG_traffic = trafic2023_ratio_rer[trafic2023_ratio_rer['LIBELLE_ARRET'] == 'BUNO GIRONVILLE']\n",
    "MP_traffic = trafic2023_ratio_rer[trafic2023_ratio_rer['LIBELLE_ARRET'] == 'MASSY PALAISEAU']\n",
    "\n",
    "CDG = CDG.merge(CDG_traffic, left_on=['day_type', 'hour'], right_on=['CAT_JOUR', 'start_hour'], how='left')\n",
    "SGP = SGP.merge(SGP_traffic, left_on=['day_type', 'hour'], right_on=['CAT_JOUR', 'start_hour'], how='left')\n",
    "BL = BL.merge(BL_traffic, left_on=['day_type', 'hour'], right_on=['CAT_JOUR', 'start_hour'], how='left')\n",
    "SL = SL.merge(SL_traffic, left_on=['day_type', 'hour'], right_on=['CAT_JOUR', 'start_hour'], how='left')\n",
    "APK = APK.merge(APK_traffic, left_on=['day_type', 'hour'], right_on=['CAT_JOUR', 'start_hour'], how='left')\n",
    "CLH = CLH.merge(CLH_traffic, left_on=['day_type', 'hour'], right_on=['CAT_JOUR', 'start_hour'], how='left')\n",
    "GBG = GBG.merge(GBG_traffic, left_on=['day_type', 'hour'], right_on=['CAT_JOUR', 'start_hour'], how='left')\n",
    "MP = MP.merge(MP_traffic, left_on=['day_type', 'hour'], right_on=['CAT_JOUR', 'start_hour'], how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ad358c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Drop uneccesary columns\n",
    "dfs = [CDG, SGP, BL, SL, APK, CLH, GBG, MP]\n",
    "\n",
    "for df in dfs:\n",
    "    df.drop(columns=['stop_reference', 'line_ref', 'day', 'date', 'Nom de la période', 'nom_jour_ferie', 'holiday_type', 'hour', 'CODE_STIF_TRNS', 'LIBELLE_ARRET', 'CAT_JOUR', 'TRNC_HORR_60', 'start_hour', 'end_hour'], inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dc91dfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#To csv\n",
    "for df, name in zip(dfs, ['CDG', 'SGP', 'BL', 'SL', 'APK', 'CLH', 'GBG', 'MP']):\n",
    "    df.to_csv(f'./Stations/{name}.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b700f024-33d9-4181-8954-9ea5fdef259b",
   "metadata": {},
   "outputs": [],
   "source": [
    "SL.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "912b7769-bb18-46fd-a199-8e2dcb5a2584",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
